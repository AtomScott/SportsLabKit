{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Evaluation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](ここに以下の書式でURLを入れる) \n",
    "[![github](https://badgen.net/badge/:status/View%20On%20Github/black?icon=github&label)](https://github.com/AtomScott/SoccerTrack)\n",
    "[![badge](https://img.shields.io/badge/launch-binder-579ACA.svg?logo=data:image/png;base64,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)](https://)\n",
    "\n",
    "---\n",
    "\n",
    "This is a work in progress tutorial, more to come soon!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Object Tracking (MOT) is notoriously difficult to evaluate accurately. While commonly used metrics such as MOTA tend to overemphasize accurate detection, IDF1 and AssA tend to overemphasize association quality. Consequently, since its proposal, Higher-Order Tracking Accuracy (HOTA) has been adopted as the primary metric in several recent benchmarks (BDD100K, KITTI, DanceTrack). HOTA seeks to balance detection and association by combining a DetA and an AssA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soccertrack\n",
    "from soccertrack import Camera\n",
    "\n",
    "dataset_path = soccertrack.datasets.get_path('top_view')\n",
    "path_to_csv = sorted(dataset_path.glob('annotations/*.csv'))[0]\n",
    "\n",
    "bbdf = soccertrack.load_df(path_to_csv) # We will use this as ground truth\n",
    "bbdf_gt = bbdf[:5]\n",
    "bbdf_track = bbdf[1:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTA\n",
    "\n",
    "**Multiple Object Tracking Accuracy (MOTA)** is a measure of the overall accuracy of the tracker. This metric has been widely adopted in the literature and considers three types of errors: \n",
    "\n",
    "1. missed detections\n",
    "2. false positives\n",
    "3. identity switches\n",
    "\n",
    "The MOTA score is calculated as the sum of these three types of errors, normalized by the total number of ground truth objects. A higher MOTA score indicates a better performance of the tracker.\n",
    "\n",
    "\\begin{equation*}\n",
    "MOTA = 1 - \\frac{ \\sum_{t} (m_t + fp_t + mme_t)}{\\sum_{{ }_t} {g_t}} \n",
    "\\end{equation*}\n",
    "\n",
    "* $m_t$:  The number of misses at time $t$\n",
    "* $fp_t$:  The number of false positives\n",
    "* $mme_t$:  The number of identity switches\n",
    "* $g_t$: The number of objects present at time $t$\n",
    "\n",
    "$MOTA$ has several limitations. For example, it overemphasizes the importance of accurate object detection, and does not take into account the association of objects over time. While the maximum possible $MOTA$ score is 1, there is no fixed minimum value. \n",
    "\n",
    "\n",
    "To use MOTA in your own code, you can use `soccertrack.metrics.mota_score()`.\n",
    "\n",
    "The following values are calculated and reported by `soccertrack.metrics.mota_score()`:\n",
    "\n",
    "* **Multiple Object Tracking Precision (MOTP)** is a measure of the precision of the tracker in terms of the location of the tracked objects. It is calculated as the average distance between the predicted and ground truth object locations. A higher MOTP score indicates better performance.\n",
    "\n",
    "* **Most Tracked Objects (MT)** is the number of ground truth objects that are successfully tracked for at least 50% of the frames in which they appear. This metric gives an indication of how well the tracker is able to maintain object tracks.\n",
    "\n",
    "* **Most Lost Objects (ML)** is the number of ground truth objects that are not successfully tracked for at least 50% of the frames in which they appear. This metric gives an indication of how well the tracker is able to maintain object tracks.\n",
    "\n",
    "* **Fragmentation** is a measure of how many times the tracker switches between different ground truth objects. A higher fragmentation score indicates worse performance.\n",
    "\n",
    "* **Id-Switches** is the number of times the tracker switches the identity of a ground truth object with another ground truth object. This metric gives an indication of how well the tracker is able to maintain object identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n",
      "{'MOTA': 0.7282608695652174, 'MOTP': 0.9825878405605193, 'MODA': 0.7282608695652174, 'CLR_Re': 0.9891304347826086, 'CLR_Pr': 0.7913043478260869, 'MTR': 0.9565217391304348, 'PTR': 0.043478260869565216, 'MLR': 0.0, 'sMOTA': 0.7110379727283397, 'CLR_F1': 0.8792270531400966, 'FP_per_frame': 4.8, 'MOTAL': 0.7282608695652174, 'MOTP_sum': 89.41549349100725, 'CLR_TP': 91, 'CLR_FN': 1, 'CLR_FP': 24, 'IDSW': 0, 'MT': 22, 'PT': 1, 'ML': 0, 'Frag': 0.0, 'CLR_Frames': 5}\n"
     ]
    }
   ],
   "source": [
    "from soccertrack.metrics import mota_score\n",
    "\n",
    "mota = mota_score(bbdf_gt, bbdf_track)\n",
    "print(mota)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOTA\n",
    "\n",
    "Higher-Order Tracking Accuracy (HOTA) is a metric that seeks to balance detection and association quality. It is calculated as the product of the Detection Accuracy (DetA) and the Association Accuracy (AssA). The DetA is calculated as the average of the detection accuracy for each class. The AssA is calculated as the average of the association accuracy for each class. The HOTA score is calculated as the product of the DetA and the AssA. A higher HOTA score indicates a better performance of the tracker.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "HOTA_\\alpha = \\sqrt{\\frac{\\sum_{c\\in\\{TP\\}} \\textit{A}(c)}{\\lvert TP \\rvert + \\lvert FN \\rvert + \\lvert FP \\rvert}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "A(c) = \\frac{\\lvert TPA(c) \\rvert}{\\lvert TPA(c) \\rvert + \\lvert FNA(c) \\rvert + \\lvert FPA(c) \\rvert}\n",
    "\\end{equation*}\n",
    "\n",
    "* $A(c):$ Measures how similar predicted trajectory and ground-truth trajectory are.  \\\\\n",
    "* $TP:$ True Positive. A ground truth detection and predicted detection are matched together given that $S \\geq \\alpha$. $S$ is the localization similarity and  $\\alpha$ is the threshold.\\\\\n",
    "* $FN:$ False Negative. A ground truth detection that was missed\\\\\n",
    "* $FP:$ False Positive. A predicted detection with no respective ground truth detection.\\\\\n",
    "* $TPA:$ True Positive Association. The set of True Positives that have the same ground truth IDs and the same prediction ID as a given $TP c$.\n",
    "\n",
    "\\begin{align*}\n",
    "TPA&(c) = \\{k\\}, \\\\\n",
    "&k \\in \\{TP|prID(k) = prID(c) \\wedge gtID(c) = gtID(c)\\}\n",
    "\\end{align*}\n",
    "\n",
    "* $FNA:$ The set of ground truth detections with the same ground truth ID as a given $TP c$. However, these detections were assigned a prediction ID different from $c$ or none at all.\n",
    "\n",
    "\\begin{align*}\n",
    "FNA&(c) = \\{k\\}, \\\\\n",
    "k & \\left. \\in\n",
    "\\begin{aligned}\n",
    "& \\{TP \\,|\\, prID(k) \\ne prID(c) \\wedge gtID(k) =  gtID(c)\\} \\\\\n",
    "& \\cup \\{FN \\,|\\, gtID(k) = gtID(c)\\} \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "* $FPA:$ The set of predicted detections with the same prediction ID as a given $TP c$. However, these detections were assigned a ground-truth  ID different from $c$ or none at all.\n",
    "\n",
    "\\begin{align*}\n",
    "FPA&(c) = \\{k\\}, \\\\\n",
    "k & \\left. \\in\n",
    "\\begin{aligned}\n",
    "    & \\{TP \\,|\\,prID(k) = prID(c)\\wedge gtID(k) \\neq gtID(c)\\} \\\\\n",
    "    & \\cup \\{FP \\,|\\, prID(k) = prID(c)\\} \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{align*}\n",
    "\n",
    "The $HOTA_\\alpha$ score represents the value of $HOTA$ calculated for a specific value of $\\alpha$. In order to obtain the final $HOTA$ score, it is necessary to calculate $HOTA_\\alpha$ for a range of values of $\\alpha$ from 0 to 1 and then take the average of these values. This allows for a more thorough evaluation of the performance of multiple object tracking systems using the $HOTA$ metric.\n",
    "\n",
    "\\begin{align*}\n",
    "HOTA =  \\int_{0}^{1} HOTA\\alpha \\:d\\alpha \\approx \\frac{1}{19} \\sum_{\\alpha \\in \\left \\{\n",
    "\\begin{aligned}\n",
    "& 0.05, 0.1,\\\\\n",
    "& ...0.9,0.95\n",
    "\\end{aligned} \\right \\} } HOTA\\alpha\n",
    "\\end{align*}\n",
    "\n",
    "To use HOTA in your own code, you can use `soccertrack.metrics.hota_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 115) (4, 115)\n",
      "TeamID             0                                                   \\\n",
      "PlayerID           0                                        1           \n",
      "Attributes bb_height bb_left   bb_top bb_width conf bb_height bb_left   \n",
      "frame                                                                   \n",
      "1                NaN     NaN      NaN      NaN  NaN       NaN     NaN   \n",
      "2               39.0  801.75  1052.25     39.0  1.0      39.0  1464.0   \n",
      "3               39.0  802.50  1053.50     39.0  1.0      39.0  1464.0   \n",
      "4               39.0  803.25  1054.75     39.0  1.0      39.0  1464.0   \n",
      "5               39.0  804.00  1056.00     39.0  1.0      39.0  1464.0   \n",
      "\n",
      "TeamID                                 ...         1               \\\n",
      "PlayerID                               ...         9                \n",
      "Attributes       bb_top bb_width conf  ... bb_height      bb_left   \n",
      "frame                                  ...                          \n",
      "1                   NaN      NaN  NaN  ...       NaN          NaN   \n",
      "2           1079.692308     39.0  1.0  ...      39.0  2536.857143   \n",
      "3           1079.384615     39.0  1.0  ...      39.0  2536.714286   \n",
      "4           1079.076923     39.0  1.0  ...      39.0  2536.571429   \n",
      "5           1078.769231     39.0  1.0  ...      39.0  2536.428571   \n",
      "\n",
      "TeamID                                     BALL                             \\\n",
      "PlayerID                                   BALL                              \n",
      "Attributes       bb_top bb_width conf bb_height  bb_left   bb_top bb_width   \n",
      "frame                                                                        \n",
      "1                   NaN      NaN  NaN       NaN      NaN      NaN      NaN   \n",
      "2           1145.714286     39.0  1.0      9.75  1910.75  1073.75     9.75   \n",
      "3           1145.428571     39.0  1.0      9.50  1905.50  1074.50     9.50   \n",
      "4           1145.142857     39.0  1.0      9.25  1900.25  1075.25     9.25   \n",
      "5           1144.857143     39.0  1.0      9.00  1895.00  1076.00     9.00   \n",
      "\n",
      "TeamID           \n",
      "PlayerID         \n",
      "Attributes conf  \n",
      "frame            \n",
      "1           NaN  \n",
      "2           1.0  \n",
      "3           1.0  \n",
      "4           1.0  \n",
      "5           1.0  \n",
      "\n",
      "[5 rows x 115 columns]\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22]), array([], dtype=float64), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22])]\n",
      "(23, 23)\n",
      "(0,)\n",
      "(23, 23)\n",
      "(23, 23)\n",
      "(23, 23)\n"
     ]
    }
   ],
   "source": [
    "from soccertrack.metrics.tracking_preprocess import to_mot_eval_format\n",
    "\n",
    "print(bbdf_gt.shape, bbdf_track.shape)\n",
    "x = to_mot_eval_format(bbdf_gt, bbdf_track)\n",
    "for i in range(len(x['similarity_scores'])):\n",
    "    print(x['similarity_scores'][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msoccertrack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m hota_score\n\u001b[0;32m----> 3\u001b[0m hota \u001b[39m=\u001b[39m hota_score(bbdf_gt, bbdf_track)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(hota)\n",
      "File \u001b[0;32m~/Github/SoccerTrack/soccertrack/metrics/hota.py:98\u001b[0m, in \u001b[0;36mhota_score\u001b[0;34m(bboxes_track, bboxes_gt)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m t, (gt_ids_t, tracker_ids_t, gt_det_t, tracker_det_t) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[39mzip\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mgt_ids\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mtracker_ids\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mgt_dets\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mtracker_dets\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     93\u001b[0m ):\n\u001b[1;32m     94\u001b[0m     \u001b[39m# Count the potential matches between ids in each timestep\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# These are normalised, weighted by the match similarity.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     similarity \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39msimilarity_scores\u001b[39m\u001b[39m\"\u001b[39m][t]\n\u001b[1;32m     97\u001b[0m     sim_iou_denom \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 98\u001b[0m         similarity\u001b[39m.\u001b[39;49msum(\u001b[39m0\u001b[39;49m)[np\u001b[39m.\u001b[39;49mnewaxis, :]\n\u001b[1;32m     99\u001b[0m         \u001b[39m+\u001b[39m similarity\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)[:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m    100\u001b[0m         \u001b[39m-\u001b[39m similarity\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     sim_iou \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(similarity)\n\u001b[1;32m    103\u001b[0m     sim_iou_mask \u001b[39m=\u001b[39m sim_iou_denom \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mfinfo(\u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39meps\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from soccertrack.metrics import hota_score\n",
    "\n",
    "hota = hota_score(bbdf_gt, bbdf_track)\n",
    "print(hota)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AssA\n",
    "\n",
    "The Association Accuracy score reflects the average alignment between matching trajectories and focuses on association errors, which occur when a single object in the ground truth is assigned two different predicted detections or a single predicted detection is assigned to two different ground truth objects. According to the MOT Benchmark, the Association Accuracy score is calculated as the average of the association Jaccard index over all matching detections and is then averaged over the localization threshold.\n",
    "\n",
    "\\begin{equation*}\n",
    "AssA\\alpha = \\frac{1}{\\lvert TP \\rvert} \\sum_{c\\in\\{TP\\}} A(c)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# AssA is calculated within hota_score\n",
    "print(hota['AssA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DetA\n",
    "Detection Accuracy is a measure of the performance of multiple object tracking systems that evaluates the accuracy of object detection. According to the MOT Benchmark, the Detection Accuracy score is calculated as the Detection Jaccard Index averaged over the localization threshold.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "DetA_\\alpha = \\frac{\\lvert TP \\rvert}{\\lvert TP \\rvert + \\lvert FN \\rvert+ \\lvert FP \\rvert}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# DetA is calculated within hota_score\n",
    "print(hota['DetA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF1\n",
    "The Identification Metric differs from others, such as $MOTA$, in that it focuses on mapping predicted trajectories with actual trajectories rather than performing bijective mapping at the detection level. Some researchers prefer this approach due to its emphasis on association over detection accuracy.\n",
    "\n",
    "\\begin{equation*}\n",
    "ID-Recall = \\frac{\\lvert IDTP \\rvert }{\\lvert IDTP \\rvert + \\lvert IDFN \\rvert }\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "ID-Precision = \\frac{\\lvert IDTP \\rvert }{\\lvert IDTP \\rvert + \\lvert IDFP \\rvert } \n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "IDF1 = \\frac{\\lvert IDTP \\rvert }{\\lvert IDTP \\rvert +0.5\\lvert IDFP \\rvert + 0.5\\lvert IDFN \\rvert }\n",
    "\\end{equation*}\n",
    "\n",
    "* $IDTP$: Identity True Positive. The predicted object trajectory and ground truth object trajectory match.\\\\\n",
    "* $IDFN$: Identify False Negative. Any ground truth detection that went undetected and has an unmatched trajectory.\\\\\n",
    "* $IDFP$: Identity False Positive. Any predicted detection that is false.\n",
    "\n",
    "$IDF1$ metric is not without its flaws. The best unique bijective mapping may not necessarily result in the best alignment between predicted and actual trajectories. This can lead to a decrease in the $IDF1$ score even in the presence of correct detections. Additionally, many unmatched trajectories can also lead to decreased scores. This can incentivize researchers to prioritize increasing the number of unique detections over making accurate associations.\n",
    "\n",
    "\n",
    "To use IDF1 in your own code, you can use `soccertrack.metrics.idf1_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing for MOT evaluation: 100%|██████████| 900/900 [00:24<00:00, 36.82it/s]\n",
      "Preprocessing for MOT evaluation: 100%|██████████| 900/900 [00:24<00:00, 36.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IDF1': 1.0, 'IDR': 1.0, 'IDP': 1.0, 'IDTP': 20700, 'IDFN': 0, 'IDFP': 0}\n"
     ]
    }
   ],
   "source": [
    "from soccertrack.metrics import identity_score\n",
    "\n",
    "identity = identity_score(bbdf_gt, bbdf_track)\n",
    "print(identity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccertrack-KNSgM8DD-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977a6a4298a6f74f911a0aa33bb1949bba79863f90698d4cbd5dc1167a3d3b34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
