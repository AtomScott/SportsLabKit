{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Model\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](„Åì„Åì„Å´‰ª•‰∏ã„ÅÆÊõ∏Âºè„ÅßURL„ÇíÂÖ•„Çå„Çã) \n",
    "[![github](https://badgen.net/badge/:status/View%20On%20Github/black?icon=github&label)](https://github.com/AtomScott/SoccerTrack)\n",
    "[![badge](https://img.shields.io/badge/launch-binder-579ACA.svg?logo=data:image/png;base64,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)](https://)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soccertrack as st\n",
    "\n",
    "dataset_path = st.datasets.get_path(\"wide_view\")\n",
    "path_to_csv = sorted(dataset_path.glob(\"annotations/*.csv\"))[0]\n",
    "path_to_mp4 = sorted(dataset_path.glob(\"videos/*.mp4\"))[0]\n",
    "\n",
    "root = st.utils.get_git_root()\n",
    "cam = st.Camera(path_to_mp4)\n",
    "\n",
    "# let's load the first frame of the video\n",
    "frame = cam.get_frame(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseTorchReIDModel', 'MLFN', 'MobileNetV2_x1_0', 'MobileNetV2_x1_4', 'OSNet_ain_x0_25', 'OSNet_ain_x0_5', 'OSNet_ain_x0_75', 'OSNet_ain_x1_0', 'OSNet_ibn_x1_0', 'OSNet_x0_25', 'OSNet_x0_5', 'OSNet_x0_75', 'OSNet_x1_0', 'ResNet50', 'ResNet50_fc512', 'ShuffleNet']\n"
     ]
    }
   ],
   "source": [
    "st.image_model.show_available_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model\n",
    "\n",
    "You can load a model by using the `load` function. The function searches subclasses of `BaseImageModel` for a match with the given name. If a match is found, an instance of the model is returned. If no match is found, a warning is logged and the function returns None. \n",
    "\n",
    "There are three arguments to the `load` function:\n",
    "* model_name: The name of the model to load. This is the name of the class, not the name of the file. It is not case sensitive.\n",
    "* model_config: A dictionary containing the configuration for the model.\n",
    "* inference_config: A dictionary containing the configuration for the inference engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mshow_inference_config:0215  üí¨| Inference configuration: \u001b[0m\n",
      "\u001b[36mshow_model_config:0210  üí¨| Model configuration: \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   name: resnet50_fc512 \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   path: https://drive.google.com/file/d/1fDJLcz4O5wxNSUvImIIjoaIF9u1Rwaud/view?usp=sharing \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   device: cpu \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   image_size: (256, 128) \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   pixel_mean: [0.485, 0.456, 0.406] \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   pixel_std: [0.229, 0.224, 0.225] \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   pixel_norm: True \u001b[0m\n",
      "\u001b[36mshow_model_config:0212  üí¨|   verbose: False \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "im_model = st.image_model.load(model_name=\"ResNet50_fc512\")\n",
    "\n",
    "# You can check the current configuration with the following methods:\n",
    "im_model.show_inference_config()\n",
    "im_model.show_model_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Embedding on a single image\n",
    "\n",
    "The input to the model should be flexible. It accepts numpy.ndarray, torch.Tensor, pathlib Path, string file, PIL Image, or a list of any of these. All inputs will be converted to a list of numpy arrays representing the images.\n",
    "\n",
    "The output of the model is expected to be a two dimensional numpy array. The first dimension is the number of images in the input. The second dimension is the number of features in the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = im_model(frame)\n",
    "print(x.shape, x.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run detection on multiple images (batched input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = im_model([frame, frame, frame, frame])\n",
    "print(x.shape, x.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a Detection Object\n",
    "\n",
    "One common use case for image embedding in tracking is to extract image features from a detected bounding box. The `Detection` object is a simple class that contains a bounding box and an image. The `Detection` object can be used as input to the `embed_detections` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/atom/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2022-4-15 torch 1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bbox_left    bbox_top  bbox_width  bbox_height      conf  class\n",
      "0  3466.608887  796.144043   57.977783   102.286011  0.521880    0.0\n",
      "1  2834.737305  589.656494   26.984131    55.704590  0.398783    0.0\n",
      "2  3062.117920  600.184692   30.535645    46.639404  0.337920    0.0\n",
      "(3, 512)\n"
     ]
    }
   ],
   "source": [
    "det_model = st.detection_model.load(model_name=\"yolov5s\")\n",
    "detections = det_model(frame, size=960)[0]\n",
    "\n",
    "embeddings = im_model.embed_detections(detections, frame)\n",
    "\n",
    "print(detections.to_df())\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a custom model\n",
    "\n",
    "Running a model with a custom architecture is easy. You just need to create a subclass of BaseImageModel and implement the `load` and `forward` methods, and then add `ConfigTemplates` for your model. See how this is done in the `BaseTorchReIDModel` model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.11, pytest-7.2.1, pluggy-1.0.0 -- /Users/atom/Library/Caches/pypoetry/virtualenvs/soccertrack-KNSgM8DD-py3.10/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/atom/Github/SoccerTrack\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 0 items                                                              \u001b[0m\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m =============================\u001b[0m\n",
      "\u001b[31mERROR: file or directory not found: /Users/atom/Github/SoccerTrack/tests/test_image_model\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!poetry run pytest {root}/tests/test_image_model --verbose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('soccertrack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86fd77fde5270c410244ce73bc0b68c8749dec758c13de6d70fa5ce033ba65ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
