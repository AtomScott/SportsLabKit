{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" run_mots.py\n",
    "\n",
    "Run example:\n",
    "run_mots.py --USE_PARALLEL False --METRICS Hota --TRACKERS_TO_EVAL TrackRCNN\n",
    "\n",
    "Command Line Arguments: Defaults, # Comments\n",
    "    Eval arguments:\n",
    "        'USE_PARALLEL': False,\n",
    "        'NUM_PARALLEL_CORES': 8,\n",
    "        'BREAK_ON_ERROR': True,\n",
    "        'PRINT_RESULTS': True,\n",
    "        'PRINT_ONLY_COMBINED': False,\n",
    "        'PRINT_CONFIG': True,\n",
    "        'TIME_PROGRESS': True,\n",
    "        'OUTPUT_SUMMARY': True,\n",
    "        'OUTPUT_DETAILED': True,\n",
    "        'PLOT_CURVES': True,\n",
    "    Dataset arguments:\n",
    "        'GT_FOLDER': os.path.join(code_path, 'data/gt/mot_challenge/'),  # Location of GT data\n",
    "        'TRACKERS_FOLDER': os.path.join(code_path, 'data/trackers/mot_challenge/'),  # Trackers location\n",
    "        'OUTPUT_FOLDER': None,  # Where to save eval results (if None, same as TRACKERS_FOLDER)\n",
    "        'TRACKERS_TO_EVAL': None,  # Filenames of trackers to eval (if None, all in folder)\n",
    "        'CLASSES_TO_EVAL': ['pedestrian'],  # Valid: ['pedestrian']\n",
    "        'SPLIT_TO_EVAL': 'train',  # Valid: 'train', 'test'\n",
    "        'INPUT_AS_ZIP': False,  # Whether tracker input files are zipped\n",
    "        'PRINT_CONFIG': True,  # Whether to print current config\n",
    "        'TRACKER_SUB_FOLDER': 'data',  # Tracker files are in TRACKER_FOLDER/tracker_name/TRACKER_SUB_FOLDER\n",
    "        'OUTPUT_SUB_FOLDER': '',  # Output files are saved in OUTPUT_FOLDER/tracker_name/OUTPUT_SUB_FOLDER\n",
    "        'SEQMAP_FOLDER': None,  # Where seqmaps are found (if None, GT_FOLDER/seqmaps)\n",
    "        'SEQMAP_FILE': None,  # Directly specify seqmap file (if none use seqmap_folder/MOTS-split_to_eval)\n",
    "        'SEQ_INFO': None,  # If not None, directly specify sequences to eval and their number of timesteps\n",
    "        'GT_LOC_FORMAT': '{gt_folder}/{seq}/gt/gt.txt',  # '{gt_folder}/{seq}/gt/gt.txt'\n",
    "        'SKIP_SPLIT_FOL': False,    # If False, data is in GT_FOLDER/MOTS-SPLIT_TO_EVAL/ and in\n",
    "                                    # TRACKERS_FOLDER/MOTS-SPLIT_TO_EVAL/tracker/\n",
    "                                    # If True, then the middle 'MOTS-split' folder is skipped for both.\n",
    "    Metric arguments:\n",
    "        'METRICS': ['HOTA','CLEAR', 'Identity', 'VACE', 'JAndF']\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from multiprocessing import freeze_support\n",
    "sys.path.append('../')\n",
    "import trackeval  # noqa: E402\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def hota_score(data):\n",
    "\n",
    "    # Initialise results\n",
    "    res = {}\n",
    "\n",
    "    plottable = True\n",
    "    array_labels = np.arange(0.05, 0.99, 0.05)\n",
    "    integer_array_fields = ['HOTA_TP', 'HOTA_FN', 'HOTA_FP']\n",
    "    float_array_fields = ['HOTA', 'DetA', 'AssA', 'DetRe', 'DetPr', 'AssRe', 'AssPr', 'LocA', 'RHOTA']\n",
    "    float_fields = ['HOTA(0)', 'LocA(0)', 'HOTALocA(0)']\n",
    "    fields = float_array_fields + integer_array_fields + float_fields\n",
    "    summary_fields = float_array_fields + float_fields\n",
    "\n",
    "\n",
    "    for field in float_array_fields + integer_array_fields:\n",
    "        res[field] = np.zeros((len(array_labels)), dtype=np.float)\n",
    "    for field in float_fields:\n",
    "        res[field] = 0\n",
    "\n",
    "    #tracker や gt シーケンスが空の場合、素早く結果を返す。\n",
    "    if data['num_tracker_dets'] == 0:\n",
    "        res['HOTA_FN'] = data['num_gt_dets'] * np.ones((len(array_labels)), dtype=np.float)\n",
    "        res['LocA'] = np.ones((len(array_labels)), dtype=np.float)\n",
    "        res['LocA(0)'] = 1.0\n",
    "        return res\n",
    "    if data['num_gt_dets'] == 0:\n",
    "        res['HOTA_FP'] = data['num_tracker_dets'] * np.ones((len(array_labels)), dtype=np.float)\n",
    "        res['LocA'] = np.ones((len(array_labels)), dtype=np.float)\n",
    "        res['LocA(0)'] = 1.0\n",
    "        return res\n",
    "\n",
    "    #グローバルなアソシエーションをカウント\n",
    "    potential_matches_count = np.zeros((data['num_gt_ids'], data['num_tracker_ids']))\n",
    "    gt_id_count = np.zeros((data['num_gt_ids'], 1))\n",
    "    tracker_id_count =  np.zeros((1, data['num_tracker_ids']))\n",
    "\n",
    "    #最初に各タイムステップをループして、グローバルトラック情報を蓄積\n",
    "    for t, (gt_ids_t, tracker_ids_t) in enumerate(zip(data['gt_ids'], data['tracker_ids'])):\n",
    "        # Count the potential matches between ids in each timestep\n",
    "        # These are normalised, weighted by the match similarity.\n",
    "        similarity = data['similarity_scores'][t] #similarityは、gtとtrackerのボックスの重なりの割合\n",
    "        sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n",
    "\n",
    "        sim_iou = np.zeros_like(similarity)\n",
    "        sim_iou_mask = sim_iou_denom > 0 + np.finfo('float').eps\n",
    "        sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n",
    "        potential_matches_count[gt_ids_t[:, np.newaxis], tracker_ids_t[np.newaxis, :]] += sim_iou\n",
    "\n",
    "        ## gt_id と tracker_id ごとに dets の総数を計算。\n",
    "        gt_id_count[gt_ids_t] += 1\n",
    "        tracker_id_count[0, tracker_ids_t] += 1\n",
    "\n",
    "    # ID間のjaccardアライメントスコア(ユニークマッチング前)を総合的に計算\n",
    "    global_alignment_score = potential_matches_count / (gt_id_count + tracker_id_count - potential_matches_count)\n",
    "    matches_counts = [np.zeros_like(potential_matches_count) for _ in array_labels]\n",
    "\n",
    "    # Calculate scores for each timestep\n",
    "    for t, (gt_ids_t, tracker_ids_t) in enumerate(zip(data['gt_ids'], data['tracker_ids'])):\n",
    "        #タイムステップに gt_det/tracker_det が存在しないの処理\n",
    "        if len(gt_ids_t) == 0:\n",
    "            for a, alpha in enumerate(array_labels):\n",
    "                res['HOTA_FP'][a] += len(tracker_ids_t)\n",
    "            continue\n",
    "        if len(tracker_ids_t) == 0:\n",
    "            for a, alpha in enumerate(array_labels):\n",
    "                res['HOTA_FN'][a] += len(gt_ids_t)\n",
    "            continue\n",
    "\n",
    "    #HOTAを最適化するためのペアのマッチングスコアを取得\n",
    "    similarity = data['similarity_scores'][t]\n",
    "    score_mat = global_alignment_score[gt_ids_t[:, np.newaxis], tracker_ids_t[np.newaxis, :]] * similarity\n",
    "\n",
    "    # Hungarian algorithm\n",
    "    match_rows, match_cols = linear_sum_assignment(-score_mat)\n",
    "\n",
    "    for a, alpha in enumerate(array_labels):\n",
    "        actually_matched_mask = similarity[match_rows, match_cols] >= alpha - np.finfo('float').eps\n",
    "        alpha_match_rows = match_rows[actually_matched_mask]\n",
    "        alpha_match_cols = match_cols[actually_matched_mask]\n",
    "        num_matches = len(alpha_match_rows)\n",
    "        res['HOTA_TP'][a] += num_matches\n",
    "        res['HOTA_FN'][a] += len(gt_ids_t) - num_matches\n",
    "        res['HOTA_FP'][a] += len(tracker_ids_t) - num_matches\n",
    "        if num_matches > 0:\n",
    "            res['LocA'][a] += sum(similarity[alpha_match_rows, alpha_match_cols])\n",
    "            matches_counts[a][gt_ids_t[alpha_match_rows], tracker_ids_t[alpha_match_cols]] += 1\n",
    "\n",
    "        #アルファ値に対するアソシエーションスコア(AssA, AssRe, AssPr)を算出\n",
    "        #gt_id/tracker_idの組み合わせごとにスコアを計算し、検出数の平均をとる。\n",
    "        for a, alpha in enumerate(array_labels):\n",
    "            matches_count = matches_counts[a]\n",
    "            ass_a = matches_count / np.maximum(1, gt_id_count + tracker_id_count - matches_count)\n",
    "            res['AssA'][a] = np.sum(matches_count * ass_a) / np.maximum(1, res['HOTA_TP'][a])\n",
    "            ass_re = matches_count / np.maximum(1, gt_id_count)\n",
    "            res['AssRe'][a] = np.sum(matches_count * ass_re) / np.maximum(1, res['HOTA_TP'][a])\n",
    "            ass_pr = matches_count / np.maximum(1, tracker_id_count)\n",
    "            res['AssPr'][a] = np.sum(matches_count * ass_pr) / np.maximum(1, res['HOTA_TP'][a])\n",
    "    \n",
    "    # Calculate final scores\n",
    "    res['LocA'] = np.maximum(1e-10, res['LocA']) / np.maximum(1e-10, res['HOTA_TP'])\n",
    "    #Calculate sub-metric ('field') values which only depend on other sub-metric values.\n",
    "    res['DetRe'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FN'])\n",
    "    # res = self._compute_final_fields(res)\n",
    "    res['DetPr'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FP'])\n",
    "    res['DetA'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FN'] + res['HOTA_FP'])\n",
    "    res['HOTA'] = np.sqrt(res['DetA'] * res['AssA'])\n",
    "    res['RHOTA'] = np.sqrt(res['DetRe'] * res['AssA'])\n",
    "    res['HOTA(0)'] = res['HOTA'][0]\n",
    "    res['LocA(0)'] = res['LocA'][0]\n",
    "    res['HOTALocA(0)'] = res['HOTA(0)']*res['LocA(0)']\n",
    "    return res\n",
    "\n",
    "def hota_eval_sequence(seq, dataset, tracker, class_list, metrics_list):\n",
    "    \"\"\"Function for evaluating a single sequence\"\"\"\n",
    "\n",
    "    raw_data = dataset.get_raw_seq_data(tracker, seq)\n",
    "    seq_res = {}\n",
    "    for cls in class_list:\n",
    "        seq_res[cls] = {}\n",
    "        data = dataset.get_preprocessed_seq_data(raw_data, cls)\n",
    "        seq_res[cls]['HOTA'] = hota_score(data)\n",
    "        \n",
    "    return seq_res\n",
    "\n",
    "def _compute_final_fields(res):\n",
    "    \"\"\"Calculate sub-metric ('field') values which only depend on other sub-metric values.\n",
    "    This function is used both for both per-sequence calculation, and in combining values across sequences.\n",
    "    \"\"\"\n",
    "    res['DetRe'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FN'])\n",
    "    res['DetPr'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FP'])\n",
    "    res['DetA'] = res['HOTA_TP'] / np.maximum(1, res['HOTA_TP'] + res['HOTA_FN'] + res['HOTA_FP'])\n",
    "    res['HOTA'] = np.sqrt(res['DetA'] * res['AssA'])\n",
    "    res['RHOTA'] = np.sqrt(res['DetRe'] * res['AssA'])\n",
    "    res['HOTA(0)'] = res['HOTA'][0]\n",
    "    res['LocA(0)'] = res['LocA'][0]\n",
    "    res['HOTALocA(0)'] = res['HOTA(0)']*res['LocA(0)']\n",
    "    return res\n",
    "\n",
    "def _combine_sum(all_res, field):\n",
    "    \"\"\"Combine sequence results via sum\"\"\"\n",
    "    return sum([all_res[k][field] for k in all_res.keys()])\n",
    "\n",
    "def _combine_weighted_av(all_res, field, comb_res, weight_field):\n",
    "    \"\"\"Combine sequence results via weighted average\"\"\"\n",
    "    return sum([all_res[k][field] * all_res[k][weight_field] for k in all_res.keys()]) / np.maximum(1.0, comb_res[weight_field])\n",
    "\n",
    "def combine_sequences(all_res):\n",
    "    \"\"\"Combines metrics across all sequences\"\"\"\n",
    "    res = {}\n",
    "    integer_array_fields = ['HOTA_TP', 'HOTA_FN', 'HOTA_FP']\n",
    "    for field in integer_array_fields:\n",
    "        res[field] = _combine_sum(all_res, field)\n",
    "    for field in ['AssRe', 'AssPr', 'AssA']:\n",
    "        res[field] = _combine_weighted_av(all_res, field, res, weight_field='HOTA_TP')\n",
    "    loca_weighted_sum = sum([all_res[k]['LocA'] * all_res[k]['HOTA_TP'] for k in all_res.keys()])\n",
    "    res['LocA'] = np.maximum(1e-10, loca_weighted_sum) / np.maximum(1e-10, res['HOTA_TP'])\n",
    "    res = _compute_final_fields(res)\n",
    "    return res\n",
    "\n",
    "def combine_classes_class_averaged(self, all_res, ignore_empty_classes=False):\n",
    "    \"\"\"Combines metrics across all classes by averaging over the class values.\n",
    "    If 'ignore_empty_classes' is True, then it only sums over classes with at least one gt or predicted detection.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    plottable = True\n",
    "    array_labels = np.arange(0.05, 0.99, 0.05)\n",
    "    integer_array_fields = ['HOTA_TP', 'HOTA_FN', 'HOTA_FP']\n",
    "    float_array_fields = ['HOTA', 'DetA', 'AssA', 'DetRe', 'DetPr', 'AssRe', 'AssPr', 'LocA', 'RHOTA']\n",
    "    float_fields = ['HOTA(0)', 'LocA(0)', 'HOTALocA(0)']\n",
    "    fields = float_array_fields + integer_array_fields + float_fields\n",
    "    summary_fields = float_array_fields + float_fields\n",
    "\n",
    "    for field in integer_array_fields:\n",
    "        if ignore_empty_classes:\n",
    "            res[field] = _combine_sum(\n",
    "                {k: v for k, v in all_res.items()\n",
    "                    if (v['HOTA_TP'] + v['HOTA_FN'] + v['HOTA_FP'] > 0 + np.finfo('float').eps).any()}, field)\n",
    "        else:\n",
    "            res[field] = _combine_sum({k: v for k, v in all_res.items()}, field)\n",
    "\n",
    "    for field in float_fields + float_array_fields:\n",
    "        if ignore_empty_classes:\n",
    "            res[field] = np.mean([v[field] for v in all_res.values() if\n",
    "                                    (v['HOTA_TP'] + v['HOTA_FN'] + v['HOTA_FP'] > 0 + np.finfo('float').eps).any()],\n",
    "                                    axis=0)\n",
    "        else:\n",
    "            res[field] = np.mean([v[field] for v in all_res.values()], axis=0)\n",
    "    return res\n",
    "\n",
    "def combine_classes_det_averaged(self, all_res):\n",
    "    \"\"\"Combines metrics across all classes by averaging over the detection values\"\"\"\n",
    "    res = {}\n",
    "    plottable = True\n",
    "    array_labels = np.arange(0.05, 0.99, 0.05)\n",
    "    integer_array_fields = ['HOTA_TP', 'HOTA_FN', 'HOTA_FP']\n",
    "    float_array_fields = ['HOTA', 'DetA', 'AssA', 'DetRe', 'DetPr', 'AssRe', 'AssPr', 'LocA', 'RHOTA']\n",
    "    float_fields = ['HOTA(0)', 'LocA(0)', 'HOTALocA(0)']\n",
    "    fields = float_array_fields + integer_array_fields + float_fields\n",
    "    summary_fields = float_array_fields + float_fields\n",
    "\n",
    "    for field in integer_array_fields:\n",
    "        res[field] = _combine_sum(all_res, field)\n",
    "    for field in ['AssRe', 'AssPr', 'AssA']:\n",
    "        res[field] = _combine_weighted_av(all_res, field, res, weight_field='HOTA_TP')\n",
    "    loca_weighted_sum = sum([all_res[k]['LocA'] * all_res[k]['HOTA_TP'] for k in all_res.keys()])\n",
    "    res['LocA'] = np.maximum(1e-10, loca_weighted_sum) / np.maximum(1e-10, res['HOTA_TP'])\n",
    "    res = _compute_final_fields(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /home/guest/dev_repo/SoccerTrack/sandbox/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : False                         \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n",
      "\n",
      "MOTSChallenge Config:\n",
      "PRINT_CONFIG         : True                          \n",
      "GT_FOLDER            : /home/guest/dev_repo/SoccerTrack/sandbox/../data/trackeval/gt/mot_challenge/\n",
      "TRACKERS_FOLDER      : /home/guest/dev_repo/SoccerTrack/sandbox/../data/trackeval/trackers/mot_challenge/\n",
      "OUTPUT_FOLDER        : None                          \n",
      "TRACKERS_TO_EVAL     : None                          \n",
      "CLASSES_TO_EVAL      : ['pedestrian']                \n",
      "SPLIT_TO_EVAL        : train                         \n",
      "INPUT_AS_ZIP         : False                         \n",
      "TRACKER_SUB_FOLDER   : data                          \n",
      "OUTPUT_SUB_FOLDER    :                               \n",
      "TRACKER_DISPLAY_NAMES : None                          \n",
      "SEQMAP_FOLDER        : None                          \n",
      "SEQMAP_FILE          : None                          \n",
      "SEQ_INFO             : None                          \n",
      "GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt   \n",
      "SKIP_SPLIT_FOL       : False                         \n",
      "\n",
      "CLEAR Config:\n",
      "THRESHOLD            : 0.5                           \n",
      "PRINT_CONFIG         : True                          \n",
      "\n",
      "Identity Config:\n",
      "THRESHOLD            : 0.5                           \n",
      "PRINT_CONFIG         : True                          \n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "freeze_support()\n",
    "\n",
    "# Command line interface:\n",
    "default_eval_config = trackeval.Evaluator.get_default_eval_config()\n",
    "default_eval_config['DISPLAY_LESS_PROGRESS'] = False\n",
    "default_dataset_config = trackeval.datasets.MOTSChallenge.get_default_dataset_config()\n",
    "default_metrics_config = {'METRICS': ['HOTA', 'CLEAR', 'Identity']}\n",
    "config = {**default_eval_config, **default_dataset_config, **default_metrics_config}  # Merge default configs\n",
    "parser = argparse.ArgumentParser()\n",
    "for setting in config.keys():\n",
    "    if type(config[setting]) == list or type(config[setting]) == type(None):\n",
    "        parser.add_argument(\"--\" + setting, nargs='+')\n",
    "    else:\n",
    "        parser.add_argument(\"--\" + setting)\n",
    "args = parser.parse_args(args=[]).__dict__\n",
    "for setting in args.keys():\n",
    "    if args[setting] is not None:\n",
    "        if type(config[setting]) == type(True):\n",
    "            if args[setting] == 'True':\n",
    "                x = True\n",
    "            elif args[setting] == 'False':\n",
    "                x = False\n",
    "            else:\n",
    "                raise Exception('Command line parameter ' + setting + 'must be True or False')\n",
    "        elif type(config[setting]) == type(1):\n",
    "            x = int(args[setting])\n",
    "        elif type(args[setting]) == type(None):\n",
    "            x = None\n",
    "        elif setting == 'SEQ_INFO':\n",
    "            x = dict(zip(args[setting], [None]*len(args[setting])))\n",
    "        else:\n",
    "            x = args[setting]\n",
    "        config[setting] = x\n",
    "eval_config = {k: v for k, v in config.items() if k in default_eval_config.keys()}\n",
    "dataset_config = {k: v for k, v in config.items() if k in default_dataset_config.keys()}\n",
    "metrics_config = {k: v for k, v in config.items() if k in default_metrics_config.keys()}\n",
    "\n",
    "\n",
    "# Run code\n",
    "evaluator = trackeval.Evaluator(eval_config)\n",
    "dataset_list = [trackeval.datasets.MOTSChallenge(dataset_config)]\n",
    "metrics_list = []\n",
    "for metric in [trackeval.metrics.HOTA, trackeval.metrics.CLEAR, trackeval.metrics.Identity, trackeval.metrics.VACE,\n",
    "                trackeval.metrics.JAndF]:\n",
    "    if metric.get_name() in metrics_config['METRICS']:\n",
    "        metrics_list.append(metric())\n",
    "if len(metrics_list) == 0:\n",
    "    raise Exception('No metrics selected for evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<trackeval.datasets.mots_challenge.MOTSChallenge at 0x7f3878eafee0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating 1 tracker(s) on 4 sequence(s) for 1 class(es) on MOTSChallenge dataset using the following metrics: H, O, T, A\n",
      "\n",
      "\n",
      "Evaluating track_rcnn\n",
      "\n",
      "    MOTSChallenge.get_raw_seq_data(track_rcnn, MOTS20-02)                  0.9761 sec\n",
      "    MOTSChallenge.get_preprocessed_seq_data(pedestrian)                    0.2448 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5154/2033258168.py:67: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  res[field] = np.zeros((len(array_labels)), dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MOTSChallenge.get_raw_seq_data(track_rcnn, MOTS20-05)                  0.5718 sec\n",
      "    MOTSChallenge.get_preprocessed_seq_data(pedestrian)                    0.2764 sec\n",
      "    MOTSChallenge.get_raw_seq_data(track_rcnn, MOTS20-09)                  0.7616 sec\n",
      "    MOTSChallenge.get_preprocessed_seq_data(pedestrian)                    0.1839 sec\n",
      "    MOTSChallenge.get_raw_seq_data(track_rcnn, MOTS20-11)                  1.2948 sec\n",
      "    MOTSChallenge.get_preprocessed_seq_data(pedestrian)                    0.3014 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import traceback\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial\n",
    "import os\n",
    "# from . import utils\n",
    "from trackeval.utils import TrackEvalException\n",
    "# from . import _timing\n",
    "from trackeval.metrics import Count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def evaluate(self, dataset_list, metrics_list):\n",
    "\"\"\"Evaluate a set of metrics on a set of datasets\"\"\"\n",
    "# config = self.config\n",
    "\n",
    "metric_name = 'HOTA'\n",
    "# metrics_list = metrics_list + [Count()]  # Count metrics are always run\n",
    "# metric_names = trackeval.utils.validate_metrics_list(metrics_list)\n",
    "dataset_names = [dataset.get_name() for dataset in dataset_list]\n",
    "output_res = {}\n",
    "output_msg = {}\n",
    "\n",
    "for dataset, dataset_name in zip(dataset_list, dataset_names):\n",
    "    # Get dataset info about what to evaluate\n",
    "    output_res[dataset_name] = {}\n",
    "    output_msg[dataset_name] = {}\n",
    "    tracker_list, seq_list, class_list = dataset.get_eval_info()\n",
    "    print('\\nEvaluating %i tracker(s) on %i sequence(s) for %i class(es) on %s dataset using the following '\n",
    "            'metrics: %s\\n' % (len(tracker_list), len(seq_list), len(class_list), dataset_name,\n",
    "                                ', '.join(metric_name)))\n",
    "    \n",
    "    for tracker in tracker_list:\n",
    "        # if not config['BREAK_ON_ERROR'] then go to next tracker without breaking\n",
    "        try:\n",
    "        # Evaluate each sequence in parallel or in series.\n",
    "        # returns a nested dict (res), indexed like: res[seq][class][metric_name][sub_metric field]\n",
    "        # e.g. res[seq_0001][pedestrian][hota][DetA]\n",
    "            print('\\nEvaluating %s\\n' % tracker)\n",
    "            time_start = time.time()\n",
    "            if config['USE_PARALLEL']:\n",
    "                with Pool(config['NUM_PARALLEL_CORES']) as pool:\n",
    "                    _eval_sequence = partial(eval_sequence, dataset=dataset, tracker=tracker,\n",
    "                                                class_list=class_list, metrics_list=metrics_list,\n",
    "                                                metric_names=metric_names)\n",
    "                    results = pool.map(_eval_sequence, seq_list)\n",
    "                    res = dict(zip(seq_list, results))\n",
    "            else:\n",
    "                res = {}\n",
    "                for curr_seq in sorted(seq_list):\n",
    "                        res[curr_seq] = hota_eval_sequence(curr_seq, dataset, tracker, class_list, metrics_list)\n",
    "\n",
    "            # Combine results over all sequences and then over all classes\n",
    "\n",
    "            # collecting combined cls keys (cls averaged, det averaged, super classes)\n",
    "            combined_cls_keys = []\n",
    "            res['COMBINED_SEQ'] = {}\n",
    "            # combine sequences for each class\n",
    "            for c_cls in class_list:\n",
    "                res['COMBINED_SEQ'][c_cls] = {}\n",
    "                ######\n",
    "                # 変更#\n",
    "                ######\n",
    "                # for metric, metric_name in zip(metrics_list, metric_names):\n",
    "                curr_res = {seq_key: seq_value[c_cls]['HOTA'] for seq_key, seq_value in res.items() if\n",
    "                                seq_key != 'COMBINED_SEQ'}\n",
    "                res['COMBINED_SEQ'][c_cls]['HOTA'] = combine_sequences(curr_res)\n",
    "            # combine classes\n",
    "            if dataset.should_classes_combine:\n",
    "                combined_cls_keys += ['cls_comb_cls_av', 'cls_comb_det_av', 'all']\n",
    "                res['COMBINED_SEQ']['cls_comb_cls_av'] = {}\n",
    "                res['COMBINED_SEQ']['cls_comb_det_av'] = {}\n",
    "\n",
    "                # for metric, metric_name in zip(metrics_list, metric_names):\n",
    "\n",
    "                cls_res = {cls_key: cls_value[metric_name] for cls_key, cls_value in\n",
    "                                res['COMBINED_SEQ'].items() if cls_key not in combined_cls_keys}\n",
    "                res['COMBINED_SEQ']['cls_comb_cls_av'][metric_name] = \\\n",
    "                        combine_classes_class_averaged(cls_res)\n",
    "                res['COMBINED_SEQ']['cls_comb_det_av'][metric_name] = \\\n",
    "                        combine_classes_det_averaged(cls_res)\n",
    "\n",
    "            # Combine results over all sequences and then over all classes\n",
    "            if dataset.use_super_categories:\n",
    "                combined_cls_keys += ['cls_comb_cls_av', 'cls_comb_det_av', 'all']\n",
    "                res['COMBINED_SEQ']['cls_comb_cls_av'] = {}\n",
    "                res['COMBINED_SEQ']['cls_comb_det_av'] = {}\n",
    "                # for metric, metric_name in zip(metrics_list, metric_names):\n",
    "                cls_res = {cls_key: cls_value[metric_name] for cls_key, cls_value in\n",
    "                                res['COMBINED_SEQ'].items() if cls_key not in combined_cls_keys}\n",
    "                res['COMBINED_SEQ']['cls_comb_cls_av'][metric_name] = \\\n",
    "                        combine_classes_class_averaged(cls_res)\n",
    "                res['COMBINED_SEQ']['cls_comb_det_av'][metric_name] = \\\n",
    "                        combine_classes_det_averaged(cls_res)\n",
    "\n",
    "            # combine classes to super classes\n",
    "            if dataset.use_super_categories:\n",
    "                for cat, sub_cats in dataset.super_categories.items():\n",
    "                    combined_cls_keys.append(cat)\n",
    "                    res['COMBINED_SEQ'][cat] = {}\n",
    "                    # for metric, metric_name in zip(metrics_list, metric_names):\n",
    "                    cat_res = {cls_key: cls_value[metric_name] for cls_key, cls_value in\n",
    "                                    res['COMBINED_SEQ'].items() if cls_key in sub_cats}\n",
    "                    res['COMBINED_SEQ'][cat][metric_name] = combine_classes_det_averaged(cat_res)\n",
    "        \n",
    "            # # Print and output results in various formats\n",
    "            # if config['TIME_PROGRESS']:\n",
    "            #     print('\\nAll sequences for %s finished in %.2f seconds' % (tracker, time.time() - time_start))\n",
    "            # output_fol = dataset.get_output_fol(tracker)\n",
    "            # tracker_display_name = dataset.get_display_name(tracker)\n",
    "            # for c_cls in res['COMBINED_SEQ'].keys():  # class_list + combined classes if calculated\n",
    "            #     summaries = []\n",
    "            #     details = []\n",
    "            #     num_dets = res['COMBINED_SEQ'][c_cls]['Count']['Dets']\n",
    "            #     if config['OUTPUT_EMPTY_CLASSES'] or num_dets > 0:\n",
    "            #         for metric, metric_name in zip(metrics_list, metric_names):\n",
    "            #             # for combined classes there is no per sequence evaluation\n",
    "            #             if c_cls in combined_cls_keys:\n",
    "            #                 table_res = {'COMBINED_SEQ': res['COMBINED_SEQ'][c_cls][metric_name]}\n",
    "            #             else:\n",
    "            #                 table_res = {seq_key: seq_value[c_cls][metric_name] for seq_key, seq_value\n",
    "            #                                 in res.items()}\n",
    "\n",
    "            #             if config['PRINT_RESULTS'] and config['PRINT_ONLY_COMBINED']:\n",
    "            #                 dont_print = dataset.should_classes_combine and c_cls not in combined_cls_keys\n",
    "            #                 if not dont_print:\n",
    "            #                     metric.print_table({'COMBINED_SEQ': table_res['COMBINED_SEQ']},\n",
    "            #                                         tracker_display_name, c_cls)\n",
    "            #             elif config['PRINT_RESULTS']:\n",
    "            #                 metric.print_table(table_res, tracker_display_name, c_cls)\n",
    "            #             if config['OUTPUT_SUMMARY']:\n",
    "            #                 summaries.append(metric.summary_results(table_res))\n",
    "            #             if config['OUTPUT_DETAILED']:\n",
    "            #                 details.append(metric.detailed_results(table_res))\n",
    "            #             if config['PLOT_CURVES']:\n",
    "            #                 metric.plot_single_tracker_results(table_res, tracker_display_name, c_cls,\n",
    "            #                                                     output_fol)\n",
    "            #         if config['OUTPUT_SUMMARY']:\n",
    "            #             utils.write_summary_results(summaries, c_cls, output_fol)\n",
    "            #         if config['OUTPUT_DETAILED']:\n",
    "            #             utils.write_detailed_results(details, c_cls, output_fol)\n",
    "            \n",
    "            # Output for returning from function\n",
    "            output_res[dataset_name][tracker] = res\n",
    "            output_msg[dataset_name][tracker] = 'Success'\n",
    "        \n",
    "        # except KeyError:\n",
    "        #     print('key error, skipping')\n",
    "        \n",
    "        except Exception as err:\n",
    "            output_res[dataset_name][tracker] = None\n",
    "            if type(err) == TrackEvalException:\n",
    "                output_msg[dataset_name][tracker] = str(err)\n",
    "            else:\n",
    "                output_msg[dataset_name][tracker] = 'Unknown error occurred.'\n",
    "            print('Tracker %s was unable to be evaluated.' % tracker)\n",
    "            print(err)\n",
    "            traceback.print_exc()\n",
    "            if config['LOG_ON_ERROR'] is not None:\n",
    "                with open(config['LOG_ON_ERROR'], 'a') as f:\n",
    "                    print(dataset_name, file=f)\n",
    "                    print(tracker, file=f)\n",
    "                    print(traceback.format_exc(), file=f)\n",
    "                    print('\\n\\n\\n', file=f)\n",
    "\n",
    "\n",
    "    #         if config['BREAK_ON_ERROR']:\n",
    "    #             raise err\n",
    "    #         elif config['RETURN_ON_ERROR']:\n",
    "    #             return output_res, output_msg\n",
    "    \n",
    "    # return output_res, output_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pedestrian': {'HOTA': {'HOTA_TP': array([32., 32., 32., 32., 32., 32., 32., 31., 30., 30., 28., 27., 25.,\n",
       "          22., 16., 12.,  5.,  1.,  0.]),\n",
       "   'HOTA_FN': array([ 8.,  8.,  8.,  8.,  8.,  8.,  8.,  9., 10., 10., 12., 13., 15.,\n",
       "          18., 24., 28., 35., 39., 40.]),\n",
       "   'HOTA_FP': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  2.,  4.,  5.,  7.,\n",
       "          10., 16., 20., 27., 31., 32.]),\n",
       "   'AssRe': array([0.01607564, 0.01607564, 0.01607564, 0.01607564, 0.01607564,\n",
       "          0.01607564, 0.01607564, 0.01636543, 0.01653215, 0.01653215,\n",
       "          0.01676756, 0.01658343, 0.01733161, 0.0192022 , 0.01746163,\n",
       "          0.01886974, 0.03080807, 0.03030303, 0.        ]),\n",
       "   'AssPr': array([0.0240342 , 0.0240342 , 0.0240342 , 0.0240342 , 0.0240342 ,\n",
       "          0.0240342 , 0.0240342 , 0.0239606 , 0.02466267, 0.02466267,\n",
       "          0.02479594, 0.01954147, 0.02069034, 0.02144815, 0.01147926,\n",
       "          0.01281836, 0.01427178, 0.00383142, 0.        ]),\n",
       "   'AssA': array([0.00646085, 0.00646085, 0.00646085, 0.00646085, 0.00646085,\n",
       "          0.00646085, 0.00646085, 0.00648804, 0.00662715, 0.00662715,\n",
       "          0.00659068, 0.00610856, 0.00641099, 0.00693766, 0.00358958,\n",
       "          0.00352731, 0.00330655, 0.00341297, 0.        ]),\n",
       "   'LocA': array([0.7298331 , 0.7298331 , 0.7298331 , 0.7298331 , 0.7298331 ,\n",
       "          0.7298331 , 0.7298331 , 0.74146171, 0.75219714, 0.75219714,\n",
       "          0.76793032, 0.77492116, 0.78697786, 0.80385592, 0.83079987,\n",
       "          0.84598077, 0.8786089 , 0.91125158, 1.        ]),\n",
       "   'DetRe': array([0.8  , 0.8  , 0.8  , 0.8  , 0.8  , 0.8  , 0.8  , 0.775, 0.75 ,\n",
       "          0.75 , 0.7  , 0.675, 0.625, 0.55 , 0.4  , 0.3  , 0.125, 0.025,\n",
       "          0.   ]),\n",
       "   'DetPr': array([1.     , 1.     , 1.     , 1.     , 1.     , 1.     , 1.     ,\n",
       "          0.96875, 0.9375 , 0.9375 , 0.875  , 0.84375, 0.78125, 0.6875 ,\n",
       "          0.5    , 0.375  , 0.15625, 0.03125, 0.     ]),\n",
       "   'DetA': array([0.8       , 0.8       , 0.8       , 0.8       , 0.8       ,\n",
       "          0.8       , 0.8       , 0.75609756, 0.71428571, 0.71428571,\n",
       "          0.63636364, 0.6       , 0.53191489, 0.44      , 0.28571429,\n",
       "          0.2       , 0.07462687, 0.01408451, 0.        ]),\n",
       "   'HOTA': array([0.07189355, 0.07189355, 0.07189355, 0.07189355, 0.07189355,\n",
       "          0.07189355, 0.07189355, 0.07003994, 0.06880173, 0.06880173,\n",
       "          0.06476162, 0.06054036, 0.05839606, 0.05525006, 0.03202488,\n",
       "          0.02656054, 0.01570853, 0.00693325, 0.        ]),\n",
       "   'RHOTA': array([0.07189355, 0.07189355, 0.07189355, 0.07189355, 0.07189355,\n",
       "          0.07189355, 0.07189355, 0.07091003, 0.0705008 , 0.0705008 ,\n",
       "          0.06792256, 0.06421275, 0.06329982, 0.06177144, 0.03789235,\n",
       "          0.03252988, 0.02033026, 0.00923711, 0.        ]),\n",
       "   'HOTA(0)': 0.07189354823564076,\n",
       "   'LocA(0)': 0.7298330998245928,\n",
       "   'HOTALocA(0)': 0.05247029116620658}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_res['MOTSChallenge']['track_rcnn']['COMBINED_SEQ']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_atom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47bcc98a91e0639070087bf2bbd0f353a7498108652c8107efb5221696e92166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
