{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" run_bdd.py\n",
    "\n",
    "Run example:\n",
    "run_bdd.py --USE_PARALLEL False --METRICS Hota --TRACKERS_TO_EVAL qdtrack\n",
    "\n",
    "Command Line Arguments: Defaults, # Comments\n",
    "    Eval arguments:\n",
    "        'USE_PARALLEL': False,\n",
    "        'NUM_PARALLEL_CORES': 8,\n",
    "        'BREAK_ON_ERROR': True,\n",
    "        'PRINT_RESULTS': True,\n",
    "        'PRINT_ONLY_COMBINED': False,\n",
    "        'PRINT_CONFIG': True,\n",
    "        'TIME_PROGRESS': True,\n",
    "        'OUTPUT_SUMMARY': True,\n",
    "        'OUTPUT_DETAILED': True,\n",
    "        'PLOT_CURVES': True,\n",
    "    Dataset arguments:\n",
    "            'GT_FOLDER': os.path.join(code_path, 'data/gt/bdd100k/bdd100k_val'),  # Location of GT data\n",
    "            'TRACKERS_FOLDER': os.path.join(code_path, 'data/trackers/bdd100k/bdd100k_val'),  # Trackers location\n",
    "            'OUTPUT_FOLDER': None,  # Where to save eval results (if None, same as TRACKERS_FOLDER)\n",
    "            'TRACKERS_TO_EVAL': None,  # Filenames of trackers to eval (if None, all in folder)\n",
    "            'CLASSES_TO_EVAL': ['pedestrian', 'rider', 'car', 'bus', 'truck', 'train', 'motorcycle', 'bicycle'],\n",
    "            # Valid: ['pedestrian', 'rider', 'car', 'bus', 'truck', 'train', 'motorcycle', 'bicycle']\n",
    "            'SPLIT_TO_EVAL': 'val',  # Valid: 'training', 'val',\n",
    "            'INPUT_AS_ZIP': False,  # Whether tracker input files are zipped\n",
    "            'PRINT_CONFIG': True,  # Whether to print current config\n",
    "            'TRACKER_SUB_FOLDER': 'data',  # Tracker files are in TRACKER_FOLDER/tracker_name/TRACKER_SUB_FOLDER\n",
    "            'OUTPUT_SUB_FOLDER': '',  # Output files are saved in OUTPUT_FOLDER/tracker_name/OUTPUT_SUB_FOLDER\n",
    "            'TRACKER_DISPLAY_NAMES': None,  # Names of trackers to display, if None: TRACKERS_TO_EVAL\n",
    "    Metric arguments:\n",
    "        'METRICS': ['Hota','Clear', 'ID', 'Count']\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from multiprocessing import freeze_support\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import trackeval  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     # freeze_support()\n",
    "\n",
    "# Command line interface:\n",
    "default_eval_config = trackeval.Evaluator.get_default_eval_config()\n",
    "default_eval_config['PRINT_ONLY_COMBINED'] = True\n",
    "default_dataset_config = trackeval.datasets.BDD100K.get_default_dataset_config()\n",
    "default_metrics_config = {'METRICS': ['HOTA', 'CLEAR', 'Identity']}\n",
    "config = {**default_eval_config, **default_dataset_config, **default_metrics_config}  # Merge default configs\n",
    "parser = argparse.ArgumentParser()\n",
    "for setting in config.keys():\n",
    "    if type(config[setting]) == list or type(config[setting]) == type(None):\n",
    "        parser.add_argument(\"--\" + setting, nargs='+')\n",
    "    else:\n",
    "        parser.add_argument(\"--\" + setting)\n",
    "\n",
    "args = parser.parse_args(args=[]).__dict__\n",
    "for setting in args.keys():\n",
    "    if args[setting] is not None:\n",
    "        if type(config[setting]) == type(True):\n",
    "            if args[setting] == 'True':\n",
    "                x = True\n",
    "            elif args[setting] == 'False':\n",
    "                x = False\n",
    "            else:\n",
    "                raise Exception('Command line parameter ' + setting + 'must be True or False')\n",
    "        elif type(config[setting]) == type(1):\n",
    "            x = int(args[setting])\n",
    "        elif type(args[setting]) == type(None):\n",
    "            x = None\n",
    "        else:\n",
    "            x = args[setting]\n",
    "        config[setting] = x\n",
    "eval_config = {k: v for k, v in config.items() if k in default_eval_config.keys()}\n",
    "dataset_config = {k: v for k, v in config.items() if k in default_dataset_config.keys()}\n",
    "metrics_config = {k: v for k, v in config.items() if k in default_metrics_config.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRINT_CONFIG': True,\n",
       " 'GT_FOLDER': '/home/guest/dev_repo/SoccerTrack/sandbox/data/gt/bdd100k/bdd100k_val',\n",
       " 'TRACKERS_FOLDER': '/home/guest/dev_repo/SoccerTrack/sandbox/data/trackers/bdd100k/bdd100k_val',\n",
       " 'OUTPUT_FOLDER': None,\n",
       " 'TRACKERS_TO_EVAL': None,\n",
       " 'CLASSES_TO_EVAL': ['pedestrian',\n",
       "  'rider',\n",
       "  'car',\n",
       "  'bus',\n",
       "  'truck',\n",
       "  'train',\n",
       "  'motorcycle',\n",
       "  'bicycle'],\n",
       " 'SPLIT_TO_EVAL': 'val',\n",
       " 'INPUT_AS_ZIP': False,\n",
       " 'TRACKER_SUB_FOLDER': 'data',\n",
       " 'OUTPUT_SUB_FOLDER': '',\n",
       " 'TRACKER_DISPLAY_NAMES': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /home/guest/dev_repo/SoccerTrack/sandbox/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : True                          \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : True                          \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n",
      "\n",
      "BDD100K Config:\n",
      "PRINT_CONFIG         : True                          \n",
      "GT_FOLDER            : /home/guest/dev_repo/SoccerTrack/sandbox/data/gt/bdd100k/bdd100k_val\n",
      "TRACKERS_FOLDER      : /home/guest/dev_repo/SoccerTrack/sandbox/data/trackers/bdd100k/bdd100k_val\n",
      "OUTPUT_FOLDER        : None                          \n",
      "TRACKERS_TO_EVAL     : None                          \n",
      "CLASSES_TO_EVAL      : ['pedestrian', 'rider', 'car', 'bus', 'truck', 'train', 'motorcycle', 'bicycle']\n",
      "SPLIT_TO_EVAL        : val                           \n",
      "INPUT_AS_ZIP         : False                         \n",
      "TRACKER_SUB_FOLDER   : data                          \n",
      "OUTPUT_SUB_FOLDER    :                               \n",
      "TRACKER_DISPLAY_NAMES : None                          \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/guest/dev_repo/SoccerTrack/sandbox/data/gt/bdd100k/bdd100k_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23706/2732666772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrackeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBDD100K\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev_repo/SoccerTrack/sandbox/lib_eval_metrics/../trackeval/datasets/bdd100k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_fol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Get trackers to eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/guest/dev_repo/SoccerTrack/sandbox/data/gt/bdd100k/bdd100k_val'"
     ]
    }
   ],
   "source": [
    "# Run code\n",
    "evaluator = trackeval.Evaluator(eval_config)\n",
    "dataset_list = [trackeval.datasets.BDD100K(dataset_config)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_atom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47bcc98a91e0639070087bf2bbd0f353a7498108652c8107efb5221696e92166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
