{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ap, map検証 ここから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getArea(box):\n",
    "    return (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "\n",
    "def _boxesIntersect(boxA, boxB):\n",
    "    if boxA[0] > boxB[2]:\n",
    "        return False  # boxA is right of boxB\n",
    "    if boxB[0] > boxA[2]:\n",
    "        return False  # boxA is left of boxB\n",
    "    if boxA[3] < boxB[1]:\n",
    "        return False  # boxA is above boxB\n",
    "    if boxA[1] > boxB[3]:\n",
    "        return False  # boxA is below boxB\n",
    "    return True\n",
    "\n",
    "def _getIntersectionArea(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # intersection area\n",
    "    return (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "def _getUnionAreas(boxA, boxB, interArea=None):\n",
    "    area_A = _getArea(boxA)\n",
    "    area_B = _getArea(boxB)\n",
    "    if interArea is None:\n",
    "        interArea = _getIntersectionArea(boxA, boxB)\n",
    "    return float(area_A + area_B - interArea)\n",
    "\n",
    "# 11-point interpolated average precision\n",
    "def ElevenPointInterpolatedAP(rec, prec):\n",
    "    # def CalculateAveragePrecision2(rec, prec):\n",
    "    mrec = []\n",
    "    # mrec.append(0)\n",
    "    [mrec.append(e) for e in rec]\n",
    "    # mrec.append(1)\n",
    "    mpre = []\n",
    "    # mpre.append(0)\n",
    "    [mpre.append(e) for e in prec]\n",
    "    # mpre.append(0)\n",
    "    recallValues = np.linspace(0, 1, 11)\n",
    "    recallValues = list(recallValues[::-1])\n",
    "    rhoInterp = []\n",
    "    recallValid = []\n",
    "    # For each recallValues (0, 0.1, 0.2, ... , 1)\n",
    "    for r in recallValues:\n",
    "        # Obtain all recall values higher or equal than r\n",
    "        argGreaterRecalls = np.argwhere(mrec[:] >= r)\n",
    "        pmax = 0\n",
    "        # If there are recalls above r\n",
    "        if argGreaterRecalls.size != 0:\n",
    "            pmax = max(mpre[argGreaterRecalls.min():])\n",
    "        recallValid.append(r)\n",
    "        rhoInterp.append(pmax)\n",
    "    # By definition AP = sum(max(precision whose recall is above r))/11\n",
    "    ap = sum(rhoInterp) / 11\n",
    "    # Generating values for the plot\n",
    "    rvals = []\n",
    "    rvals.append(recallValid[0])\n",
    "    [rvals.append(e) for e in recallValid]\n",
    "    rvals.append(0)\n",
    "    pvals = []\n",
    "    pvals.append(0)\n",
    "    [pvals.append(e) for e in rhoInterp]\n",
    "    pvals.append(0)\n",
    "    # rhoInterp = rhoInterp[::-1]\n",
    "    cc = []\n",
    "    for i in range(len(rvals)):\n",
    "        p = (rvals[i], pvals[i - 1])\n",
    "        if p not in cc:\n",
    "            cc.append(p)\n",
    "        p = (rvals[i], pvals[i])\n",
    "        if p not in cc:\n",
    "            cc.append(p)\n",
    "    recallValues = [i[0] for i in cc]\n",
    "    rhoInterp = [i[1] for i in cc]\n",
    "    return [ap, rhoInterp, recallValues, None]\n",
    "\n",
    "\n",
    "def calc_iou(bbox_det: Tuple[int], bbox_gt: Tuple[int]) -> float:\n",
    "    \"\"\"calculate iou between two bbox\n",
    "    Args:\n",
    "        bbox_det(tuple(int, int, int, int)): bbox of detected object\n",
    "        bbox_gt(tuple(int, int, int, int)): bbox of ground truth object\n",
    " \n",
    "    Returns:\n",
    "        iou_score(float): iou_score between two bbox\n",
    "    \"\"\"\n",
    "    # if boxes dont intersect\n",
    "    if _boxesIntersect(bbox_det, bbox_gt) is False:\n",
    "        return 0\n",
    "    interArea = _getIntersectionArea(bbox_det, bbox_gt)\n",
    "    union = _getUnionAreas(bbox_det, bbox_gt, interArea=interArea)\n",
    "    # intersection over union\n",
    "    iou_score = interArea / union\n",
    "    assert iou_score >= 0\n",
    "    return iou_score\n",
    "\n",
    "def calc_ap(class_id: str, TP: List[int], FP: List[int], npos: int , ap_only: bool) -> dict:\n",
    "    \"\"\"calculate average precision\n",
    "    Args:\n",
    "        class_id(str): class id\n",
    "        TP(list[int]): true positive\n",
    "        FP(list[int]): false positive\n",
    "        npos(int): number of positive samples\n",
    "        ap_only(bool): if true, only return average precision, do not recall and precision\n",
    "\n",
    "    Returns:\n",
    "        ap_score(dict): dict about average precision score\n",
    "    \"\"\"\n",
    "    acc_FP = np.cumsum(FP)\n",
    "    acc_TP = np.cumsum(TP)\n",
    "    rec = acc_TP / npos\n",
    "    # print(rec)\n",
    "    prec = np.divide(acc_TP, (acc_FP + acc_TP))\n",
    "    # print(prec)\n",
    "    # Depending on the method, call the right implementation\n",
    "    [ap, mpre, mrec, _] = ElevenPointInterpolatedAP(rec, prec)\n",
    "    # if method == MethodAveragePrecision.EveryPointInterpolation:\n",
    "\n",
    "    if ap_only:\n",
    "        ap_score = {\n",
    "            'class': class_id,\n",
    "            'AP': ap\n",
    "            }\n",
    "        return ap_score\n",
    "    else:\n",
    "        ap_score = {\n",
    "            'class': class_id,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'AP': ap,\n",
    "            'interpolated precision': mpre,\n",
    "            'interpolated recall': mrec,\n",
    "            'total positives': npos,\n",
    "            'total TP': np.sum(TP),\n",
    "            'total FP': np.sum(FP)\n",
    "        }\n",
    "        return ap_score\n",
    "\n",
    "def calc_map(ap_score_list: (List[dict])) -> float:\n",
    "    \"\"\"calculate mean average precision\n",
    "    Args:\n",
    "        ap_score_list(list(dict)): list of dict about average precision score\n",
    "\n",
    "    Returns:\n",
    "        map_score(float): mean average precision\n",
    "    \"\"\"\n",
    "    ap_list = []\n",
    "    for ap_score_per_class in ap_score_list:\n",
    "        ap_list.append(ap_score_per_class['AP'])\n",
    "        # print(f'class: {ap_score_per_class[\"class\"]} , AP: {ap_score_per_class[\"AP\"]}')\n",
    "    map_score = np.mean(ap_list)\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy bounding box\n",
    "image_num = 5 #number of images\n",
    "bbox_offset = 0 #検出用バウンディングボックスのオフセット\n",
    "IOUThreshold = 0.5 #IOU閾値\n",
    "ap_only = True #calc_ap関数で、APのみを出力するかどうかのトリガー\n",
    "\n",
    "ap_score_list = []  # list containing metrics of each class\n",
    "groundTruths = []\n",
    "detections = []\n",
    "classes = []\n",
    "iouMax_list = []\n",
    "\n",
    "\n",
    "for image_name in range(image_num):\n",
    "    for ClassId in [0 ,32] :\n",
    "        for num in range(0, 1000, 100):\n",
    "            getImageName = f'{image_name}'\n",
    "            getClassId = f'{ClassId}'\n",
    "\n",
    "            # bbox_offset = random.randint(0, 100) #Offset of the bounding box(random) <- ハイパラ\n",
    "            # print(f'bbox_offset: {bbox_offset}')\n",
    "\n",
    "            num_gt = num\n",
    "            num_det = num + bbox_offset\n",
    "\n",
    "            x1 = (1 * num_gt , 1 * num_det)\n",
    "            y1 = (1 * num_gt , 1 * num_det)\n",
    "            x2 = (1 * num_gt + 50 , 1 * num_det+ 50)\n",
    "            y2 = (1 * num_gt + 50 , 1 * num_det+ 50)\n",
    "\n",
    "\n",
    "            getAbsoluteBoundingBox_gt = (x1[0], y1[0], x2[0], y2[0])\n",
    "            getAbsoluteBoundingBox_det = (x1[1], y1[1], x2[1], y2[1])\n",
    "\n",
    "            getConfidence = random.random()\n",
    "\n",
    "            gt_info = [getImageName, getClassId, 1, getAbsoluteBoundingBox_gt]\n",
    "            det_info = [getImageName, getClassId, getConfidence ,getAbsoluteBoundingBox_det]\n",
    "\n",
    "            if getClassId not in classes:\n",
    "                classes.append(getClassId)\n",
    "\n",
    "            groundTruths.append(gt_info)\n",
    "            # if num % 500 == 0: \n",
    "            #     continue\n",
    "            # else:\n",
    "            #     detections.append(det_info)\n",
    "            detections.append(det_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating class: 0 (50 detections)\n",
      "ap_score: {'class': '0', 'AP': 0.0875311557129739}\n",
      "Evaluating class: 32 (50 detections)\n",
      "ap_score: {'class': '32', 'AP': 0.029598308668076112}\n",
      "-----\n",
      "map_score: 0.05856473219052501\n"
     ]
    }
   ],
   "source": [
    "#calculate iou\n",
    "classes = sorted(classes)\n",
    "for class_id in classes:\n",
    "    # Get only detection of class c\n",
    "    dects = []\n",
    "    [dects.append(d) for d in detections if d[1] == class_id]\n",
    "    # Get only ground truths of class c, use filename as key\n",
    "    gts = {}\n",
    "    npos = 0\n",
    "    for g in groundTruths:\n",
    "        if g[1] == class_id:\n",
    "            npos += 1\n",
    "            gts[g[0]] = gts.get(g[0], []) + [g]\n",
    "\n",
    "    # sort detections by decreasing confidence\n",
    "    dects = sorted(dects, key=lambda conf: conf[2], reverse=True)\n",
    "    # create dictionary with amount of gts for each image\n",
    "    det = {key: np.zeros(len(gts[key])) for key in gts}\n",
    "\n",
    "    print(\"Evaluating class: %s (%d detections)\" % (str(class_id), len(dects)))\n",
    "    # Loop through detections\n",
    "    TP = np.zeros(len(dects))\n",
    "    FP = np.zeros(len(dects))\n",
    "    for d in range(len(dects)):\n",
    "        # print('dect %s => %s' % (dects[d][0], dects[d][3],))\n",
    "        # Find ground truth image\n",
    "        gt = gts[dects[d][0]] if dects[d][0] in gts else []\n",
    "        iouMax = sys.float_info.min\n",
    "        for j in range(len(gt)):\n",
    "            # print('Ground truth gt => %s' % (gt[j][3],))\n",
    "            iou_score = calc_iou(dects[d][3], gt[j][3])\n",
    "            # print('iou_output => %s' % (iou_output,))\n",
    "            if iou_score > iouMax:\n",
    "                iouMax = iou_score\n",
    "                jmax = j\n",
    "                # print(iouMax)\n",
    "                iouMax_list.append(iouMax)\n",
    "                # print(f'img_name: {dects[d][0]} , class_id: {dects[d][1]} , confidence: {dects[d][2]} , iou: {iouMax}, jmax: {jmax}')\n",
    "            # print(iou_output)\n",
    "        # Assign detection as true positive/don't care/false positive\n",
    "        if iouMax >= IOUThreshold:\n",
    "            TP[d] = 1  # count as true positive\n",
    "            det[dects[d][0]][jmax] = 1  # flag as already 'seen'\n",
    "            # print(\"TP\")\n",
    "        else:\n",
    "            FP[d] = 1  # count as false positive\n",
    "    # compute precision, recall and average precision\n",
    "\n",
    "    ap_score = calc_ap(class_id,TP, FP, npos, ap_only)\n",
    "    print(f'ap_score: {ap_score}')\n",
    "    ap_score_list.append(ap_score)\n",
    "\n",
    "map_score = calc_map(ap_score_list)\n",
    "\n",
    "print('-----')\n",
    "print(f'map_score: {map_score}')\n",
    "\n",
    "# iouMax_list\n",
    "# >>> [1.0, 1.0, ....., 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ここまで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class': '0',\n",
       "  'precision': array([1.        , 0.5       , 0.33333333, 0.5       , 0.4       ,\n",
       "         0.33333333, 0.28571429, 0.25      , 0.22222222, 0.2       ,\n",
       "         0.18181818, 0.16666667, 0.15384615, 0.14285714, 0.13333333,\n",
       "         0.125     , 0.11764706, 0.11111111, 0.10526316, 0.1       ,\n",
       "         0.0952381 , 0.09090909, 0.08695652, 0.08333333, 0.08      ,\n",
       "         0.07692308, 0.07407407, 0.07142857, 0.10344828, 0.1       ,\n",
       "         0.09677419, 0.09375   , 0.12121212, 0.11764706, 0.11428571,\n",
       "         0.11111111, 0.13513514, 0.13157895, 0.12820513, 0.125     ,\n",
       "         0.12195122, 0.11904762, 0.11627907, 0.13636364, 0.13333333,\n",
       "         0.15217391, 0.14893617, 0.14583333, 0.14285714, 0.16      ]),\n",
       "  'recall': array([0.02, 0.02, 0.02, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n",
       "         0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n",
       "         0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.06, 0.06, 0.06, 0.06, 0.08,\n",
       "         0.08, 0.08, 0.08, 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.12,\n",
       "         0.12, 0.14, 0.14, 0.14, 0.14, 0.16]),\n",
       "  'AP': 0.10545454545454545,\n",
       "  'interpolated precision': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16, 0.16, 1.0, 0],\n",
       "  'interpolated recall': [1.0,\n",
       "   0.9,\n",
       "   0.8,\n",
       "   0.7000000000000001,\n",
       "   0.6000000000000001,\n",
       "   0.5,\n",
       "   0.4,\n",
       "   0.30000000000000004,\n",
       "   0.2,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0],\n",
       "  'total positives': 50,\n",
       "  'total TP': 8.0,\n",
       "  'total FP': 42.0},\n",
       " {'class': '32',\n",
       "  'precision': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07142857, 0.06666667,\n",
       "         0.0625    , 0.05882353, 0.05555556, 0.05263158, 0.05      ,\n",
       "         0.04761905, 0.04545455, 0.04347826, 0.04166667, 0.04      ,\n",
       "         0.03846154, 0.03703704, 0.07142857, 0.10344828, 0.1       ,\n",
       "         0.09677419, 0.09375   , 0.09090909, 0.08823529, 0.08571429,\n",
       "         0.11111111, 0.10810811, 0.10526316, 0.1025641 , 0.1       ,\n",
       "         0.09756098, 0.0952381 , 0.11627907, 0.13636364, 0.13333333,\n",
       "         0.13043478, 0.12765957, 0.125     , 0.12244898, 0.12      ]),\n",
       "  'recall': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "         0.  , 0.  , 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n",
       "         0.02, 0.02, 0.02, 0.02, 0.02, 0.04, 0.06, 0.06, 0.06, 0.06, 0.06,\n",
       "         0.06, 0.06, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.1 , 0.12,\n",
       "         0.12, 0.12, 0.12, 0.12, 0.12, 0.12]),\n",
       "  'AP': 0.024793388429752063,\n",
       "  'interpolated precision': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0.13636363636363635,\n",
       "   0.13636363636363635,\n",
       "   0],\n",
       "  'interpolated recall': [1.0,\n",
       "   0.9,\n",
       "   0.8,\n",
       "   0.7000000000000001,\n",
       "   0.6000000000000001,\n",
       "   0.5,\n",
       "   0.4,\n",
       "   0.30000000000000004,\n",
       "   0.2,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.0,\n",
       "   0],\n",
       "  'total positives': 50,\n",
       "  'total TP': 6.0,\n",
       "  'total FP': 44.0}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ret = []  # list containing metrics (precision, recall, average precision) of each class\n",
    "# # List with all ground truths (Ex: [imageName,class,confidence=1, (bb coordinates XYX2Y2)])\n",
    "# groundTruths = []\n",
    "# # List with all detections (Ex: [imageName,class,confidence,(bb coordinates XYX2Y2)])\n",
    "# detections = []\n",
    "# # Get all classes\n",
    "# classes = []\n",
    "# # Loop through all bounding boxes and separate them into GTs and detections\n",
    "# for bb in boundingboxes.getBoundingBoxes():\n",
    "#     # [imageName, class, confidence, (bb coordinates XYX2Y2)]\n",
    "#     if bb.getBBType() == BBType.GroundTruth:\n",
    "#         groundTruths.append([\n",
    "#             bb.getImageName(),\n",
    "#             bb.getClassId(), 1,\n",
    "#             bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#         ])\n",
    "#     else:\n",
    "#         detections.append([\n",
    "#             bb.getImageName(),\n",
    "#             bb.getClassId(),\n",
    "#             bb.getConfidence(),\n",
    "#             bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#         ])\n",
    "#     # get class\n",
    "#     if bb.getClassId() not in classes:\n",
    "#         classes.append(bb.getClassId())\n",
    "# classes = sorted(classes)\n",
    "\n",
    "# # Precision x Recall is obtained individually by each class\n",
    "# # Loop through by classes\n",
    "# for c in classes:\n",
    "#     # Get only detection of class c\n",
    "#     dects = []\n",
    "#     [dects.append(d) for d in detections if d[1] == c]\n",
    "#     # Get only ground truths of class c, use filename as key\n",
    "#     gts = {}\n",
    "#     npos = 0\n",
    "#     for g in groundTruths:\n",
    "#         if g[1] == c:\n",
    "#             npos += 1\n",
    "#             gts[g[0]] = gts.get(g[0], []) + [g]\n",
    "\n",
    "#     # sort detections by decreasing confidence\n",
    "#     dects = sorted(dects, key=lambda conf: conf[2], reverse=True)\n",
    "#     TP = np.zeros(len(dects))\n",
    "#     FP = np.zeros(len(dects))\n",
    "#     # create dictionary with amount of gts for each image\n",
    "#     det = {key: np.zeros(len(gts[key])) for key in gts}\n",
    "\n",
    "#     # print(\"Evaluating class: %s (%d detections)\" % (str(c), len(dects)))\n",
    "#     # Loop through detections\n",
    "#     for d in range(len(dects)):\n",
    "#         # print('dect %s => %s' % (dects[d][0], dects[d][3],))\n",
    "#         # Find ground truth image\n",
    "#         gt = gts[dects[d][0]] if dects[d][0] in gts else []\n",
    "#         iouMax = sys.float_info.min\n",
    "#         for j in range(len(gt)):\n",
    "#             # print('Ground truth gt => %s' % (gt[j][3],))\n",
    "#             iou = Evaluator.iou(dects[d][3], gt[j][3])\n",
    "#             if iou > iouMax:\n",
    "#                 iouMax = iou\n",
    "#                 jmax = j\n",
    "#         # Assign detection as true positive/don't care/false positive\n",
    "#         if iouMax >= IOUThreshold:\n",
    "#             if det[dects[d][0]][jmax] == 0:\n",
    "#                 TP[d] = 1  # count as true positive\n",
    "#                 det[dects[d][0]][jmax] = 1  # flag as already 'seen'\n",
    "#                 # print(\"TP\")\n",
    "#             else:\n",
    "#                 FP[d] = 1  # count as false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Evaluator:\n",
    "#     def GetPascalVOCMetrics(self,\n",
    "#                             boundingboxes,\n",
    "#                             IOUThreshold=0.5,\n",
    "#                             method=MethodAveragePrecision.EveryPointInterpolation):\n",
    "#         \"\"\"Get the metrics used by the VOC Pascal 2012 challenge.\n",
    "#         Get\n",
    "#         Args:\n",
    "#             boundingboxes: Object of the class BoundingBoxes representing ground truth and detected\n",
    "#             bounding boxes;\n",
    "#             IOUThreshold: IOU threshold indicating which detections will be considered TP or FP\n",
    "#             (default value = 0.5);\n",
    "#             method (default = EveryPointInterpolation): It can be calculated as the implementation\n",
    "#             in the official PASCAL VOC toolkit (EveryPointInterpolation), or applying the 11-point\n",
    "#             interpolatio as described in the paper \"The PASCAL Visual Object Classes(VOC) Challenge\"\n",
    "#             or EveryPointInterpolation\"  (ElevenPointInterpolation);\n",
    "#         Returns:\n",
    "#             A list of dictionaries. Each dictionary contains information and metrics of each class.\n",
    "#             The keys of each dictionary are:\n",
    "#             dict['class']: class representing the current dictionary;\n",
    "#             dict['precision']: array with the precision values;\n",
    "#             dict['recall']: array with the recall values;\n",
    "#             dict['AP']: average precision;\n",
    "#             dict['interpolated precision']: interpolated precision values;\n",
    "#             dict['interpolated recall']: interpolated recall values;\n",
    "#             dict['total positives']: total number of ground truth positives;\n",
    "#             dict['total TP']: total number of True Positive detections;\n",
    "#             dict['total FP']: total number of False Positive detections;\n",
    "#         \"\"\"\n",
    "#         ret = []  # list containing metrics (precision, recall, average precision) of each class\n",
    "#         # List with all ground truths (Ex: [imageName,class,confidence=1, (bb coordinates XYX2Y2)])\n",
    "#         groundTruths = []\n",
    "#         # List with all detections (Ex: [imageName,class,confidence,(bb coordinates XYX2Y2)])\n",
    "#         detections = []\n",
    "#         # Get all classes\n",
    "#         classes = []\n",
    "#         # Loop through all bounding boxes and separate them into GTs and detections\n",
    "#         for bb in boundingboxes.getBoundingBoxes():\n",
    "#             # [imageName, class, confidence, (bb coordinates XYX2Y2)]\n",
    "#             if bb.getBBType() == BBType.GroundTruth:\n",
    "#                 groundTruths.append([\n",
    "#                     bb.getImageName(),\n",
    "#                     bb.getClassId(), 1,\n",
    "#                     bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#                 ])\n",
    "#             else:\n",
    "#                 detections.append([\n",
    "#                     bb.getImageName(),\n",
    "#                     bb.getClassId(),\n",
    "#                     bb.getConfidence(),\n",
    "#                     bb.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#                 ])\n",
    "#             # get class\n",
    "#             if bb.getClassId() not in classes:\n",
    "#                 classes.append(bb.getClassId())\n",
    "#         classes = sorted(classes)\n",
    "#         # Precision x Recall is obtained individually by each class\n",
    "#         # Loop through by classes\n",
    "#         for c in classes:\n",
    "#             # Get only detection of class c\n",
    "#             dects = []\n",
    "#             [dects.append(d) for d in detections if d[1] == c]\n",
    "#             # Get only ground truths of class c, use filename as key\n",
    "#             gts = {}\n",
    "#             npos = 0\n",
    "#             for g in groundTruths:\n",
    "#                 if g[1] == c:\n",
    "#                     npos += 1\n",
    "#                     gts[g[0]] = gts.get(g[0], []) + [g]\n",
    "\n",
    "#             # sort detections by decreasing confidence\n",
    "#             dects = sorted(dects, key=lambda conf: conf[2], reverse=True)\n",
    "#             TP = np.zeros(len(dects))\n",
    "#             FP = np.zeros(len(dects))\n",
    "#             # create dictionary with amount of gts for each image\n",
    "#             det = {key: np.zeros(len(gts[key])) for key in gts}\n",
    "\n",
    "#             # print(\"Evaluating class: %s (%d detections)\" % (str(c), len(dects)))\n",
    "#             # Loop through detections\n",
    "#             for d in range(len(dects)):\n",
    "#                 # print('dect %s => %s' % (dects[d][0], dects[d][3],))\n",
    "#                 # Find ground truth image\n",
    "#                 gt = gts[dects[d][0]] if dects[d][0] in gts else []\n",
    "#                 iouMax = sys.float_info.min\n",
    "#                 for j in range(len(gt)):\n",
    "#                     # print('Ground truth gt => %s' % (gt[j][3],))\n",
    "#                     iou = Evaluator.iou(dects[d][3], gt[j][3])\n",
    "#                     if iou > iouMax:\n",
    "#                         iouMax = iou\n",
    "#                         jmax = j\n",
    "#                 # Assign detection as true positive/don't care/false positive\n",
    "#                 if iouMax >= IOUThreshold:\n",
    "#                     if det[dects[d][0]][jmax] == 0:\n",
    "#                         TP[d] = 1  # count as true positive\n",
    "#                         det[dects[d][0]][jmax] = 1  # flag as already 'seen'\n",
    "#                         # print(\"TP\")\n",
    "#                     else:\n",
    "#                         FP[d] = 1  # count as false positive\n",
    "#                         # print(\"FP\")\n",
    "#                 # - A detected \"cat\" is overlaped with a GT \"cat\" with IOU >= IOUThreshold.\n",
    "#                 else:\n",
    "#                     FP[d] = 1  # count as false positive\n",
    "#                     # print(\"FP\")\n",
    "#             # compute precision, recall and average precision\n",
    "#             acc_FP = np.cumsum(FP)\n",
    "#             acc_TP = np.cumsum(TP)\n",
    "#             rec = acc_TP / npos\n",
    "#             prec = np.divide(acc_TP, (acc_FP + acc_TP))\n",
    "#             # Depending on the method, call the right implementation\n",
    "#             if method == MethodAveragePrecision.EveryPointInterpolation:\n",
    "#                 [ap, mpre, mrec, ii] = Evaluator.CalculateAveragePrecision(rec, prec)\n",
    "#             else:\n",
    "#                 [ap, mpre, mrec, _] = Evaluator.ElevenPointInterpolatedAP(rec, prec)\n",
    "#             # add class result in the dictionary to be returned\n",
    "#             r = {\n",
    "#                 'class': c,\n",
    "#                 'precision': prec,\n",
    "#                 'recall': rec,\n",
    "#                 'AP': ap,\n",
    "#                 'interpolated precision': mpre,\n",
    "#                 'interpolated recall': mrec,\n",
    "#                 'total positives': npos,\n",
    "#                 'total TP': np.sum(TP),\n",
    "#                 'total FP': np.sum(FP)\n",
    "#             }\n",
    "#             ret.append(r)\n",
    "#         return ret\n",
    "\n",
    "#     def PlotPrecisionRecallCurve(self,\n",
    "#                                  boundingBoxes,\n",
    "#                                  IOUThreshold=0.5,\n",
    "#                                  method=MethodAveragePrecision.EveryPointInterpolation,\n",
    "#                                  showAP=False,\n",
    "#                                  showInterpolatedPrecision=False,\n",
    "#                                  savePath=None,\n",
    "#                                  showGraphic=True):\n",
    "#         \"\"\"PlotPrecisionRecallCurve\n",
    "#         Plot the Precision x Recall curve for a given class.\n",
    "#         Args:\n",
    "#             boundingBoxes: Object of the class BoundingBoxes representing ground truth and detected\n",
    "#             bounding boxes;\n",
    "#             IOUThreshold (optional): IOU threshold indicating which detections will be considered\n",
    "#             TP or FP (default value = 0.5);\n",
    "#             method (default = EveryPointInterpolation): It can be calculated as the implementation\n",
    "#             in the official PASCAL VOC toolkit (EveryPointInterpolation), or applying the 11-point\n",
    "#             interpolatio as described in the paper \"The PASCAL Visual Object Classes(VOC) Challenge\"\n",
    "#             or EveryPointInterpolation\"  (ElevenPointInterpolation).\n",
    "#             showAP (optional): if True, the average precision value will be shown in the title of\n",
    "#             the graph (default = False);\n",
    "#             showInterpolatedPrecision (optional): if True, it will show in the plot the interpolated\n",
    "#              precision (default = False);\n",
    "#             savePath (optional): if informed, the plot will be saved as an image in this path\n",
    "#             (ex: /home/mywork/ap.png) (default = None);\n",
    "#             showGraphic (optional): if True, the plot will be shown (default = True)\n",
    "#         Returns:\n",
    "#             A list of dictionaries. Each dictionary contains information and metrics of each class.\n",
    "#             The keys of each dictionary are:\n",
    "#             dict['class']: class representing the current dictionary;\n",
    "#             dict['precision']: array with the precision values;\n",
    "#             dict['recall']: array with the recall values;\n",
    "#             dict['AP']: average precision;\n",
    "#             dict['interpolated precision']: interpolated precision values;\n",
    "#             dict['interpolated recall']: interpolated recall values;\n",
    "#             dict['total positives']: total number of ground truth positives;\n",
    "#             dict['total TP']: total number of True Positive detections;\n",
    "#             dict['total FP']: total number of False Negative detections;\n",
    "#         \"\"\"\n",
    "#         results = self.GetPascalVOCMetrics(boundingBoxes, IOUThreshold, method)\n",
    "#         result = None\n",
    "#         # Each resut represents a class\n",
    "#         for result in results:\n",
    "#             if result is None:\n",
    "#                 raise IOError('Error: Class %d could not be found.' % classId)\n",
    "\n",
    "#             classId = result['class']\n",
    "#             precision = result['precision']\n",
    "#             recall = result['recall']\n",
    "#             average_precision = result['AP']\n",
    "#             mpre = result['interpolated precision']\n",
    "#             mrec = result['interpolated recall']\n",
    "#             npos = result['total positives']\n",
    "#             total_tp = result['total TP']\n",
    "#             total_fp = result['total FP']\n",
    "\n",
    "#             plt.close()\n",
    "#             if showInterpolatedPrecision:\n",
    "#                 if method == MethodAveragePrecision.EveryPointInterpolation:\n",
    "#                     plt.plot(mrec, mpre, '--r', label='Interpolated precision (every point)')\n",
    "#                 elif method == MethodAveragePrecision.ElevenPointInterpolation:\n",
    "#                     # Uncomment the line below if you want to plot the area\n",
    "#                     # plt.plot(mrec, mpre, 'or', label='11-point interpolated precision')\n",
    "#                     # Remove duplicates, getting only the highest precision of each recall value\n",
    "#                     nrec = []\n",
    "#                     nprec = []\n",
    "#                     for idx in range(len(mrec)):\n",
    "#                         r = mrec[idx]\n",
    "#                         if r not in nrec:\n",
    "#                             idxEq = np.argwhere(mrec == r)\n",
    "#                             nrec.append(r)\n",
    "#                             nprec.append(max([mpre[int(id)] for id in idxEq]))\n",
    "#                     plt.plot(nrec, nprec, 'or', label='11-point interpolated precision')\n",
    "#             plt.plot(recall, precision, label='Precision')\n",
    "#             plt.xlabel('recall')\n",
    "#             plt.ylabel('precision')\n",
    "#             if showAP:\n",
    "#                 ap_str = \"{0:.2f}%\".format(average_precision * 100)\n",
    "#                 # ap_str = \"{0:.4f}%\".format(average_precision * 100)\n",
    "#                 plt.title('Precision x Recall curve \\nClass: %s, AP: %s' % (str(classId), ap_str))\n",
    "#             else:\n",
    "#                 plt.title('Precision x Recall curve \\nClass: %s' % str(classId))\n",
    "#             plt.legend(shadow=True)\n",
    "#             plt.grid()\n",
    "#             ############################################################\n",
    "#             # Uncomment the following block to create plot with points #\n",
    "#             ############################################################\n",
    "#             # plt.plot(recall, precision, 'bo')\n",
    "#             # labels = ['R', 'Y', 'J', 'A', 'U', 'C', 'M', 'F', 'D', 'B', 'H', 'P', 'E', 'X', 'N', 'T',\n",
    "#             # 'K', 'Q', 'V', 'I', 'L', 'S', 'G', 'O']\n",
    "#             # dicPosition = {}\n",
    "#             # dicPosition['left_zero'] = (-30,0)\n",
    "#             # dicPosition['left_zero_slight'] = (-30,-10)\n",
    "#             # dicPosition['right_zero'] = (30,0)\n",
    "#             # dicPosition['left_up'] = (-30,20)\n",
    "#             # dicPosition['left_down'] = (-30,-25)\n",
    "#             # dicPosition['right_up'] = (20,20)\n",
    "#             # dicPosition['right_down'] = (20,-20)\n",
    "#             # dicPosition['up_zero'] = (0,30)\n",
    "#             # dicPosition['up_right'] = (0,30)\n",
    "#             # dicPosition['left_zero_long'] = (-60,-2)\n",
    "#             # dicPosition['down_zero'] = (-2,-30)\n",
    "#             # vecPositions = [\n",
    "#             #     dicPosition['left_down'],\n",
    "#             #     dicPosition['left_zero'],\n",
    "#             #     dicPosition['right_zero'],\n",
    "#             #     dicPosition['right_zero'],  #'R', 'Y', 'J', 'A',\n",
    "#             #     dicPosition['left_up'],\n",
    "#             #     dicPosition['left_up'],\n",
    "#             #     dicPosition['right_up'],\n",
    "#             #     dicPosition['left_up'],  # 'U', 'C', 'M', 'F',\n",
    "#             #     dicPosition['left_zero'],\n",
    "#             #     dicPosition['right_up'],\n",
    "#             #     dicPosition['right_down'],\n",
    "#             #     dicPosition['down_zero'],  #'D', 'B', 'H', 'P'\n",
    "#             #     dicPosition['left_up'],\n",
    "#             #     dicPosition['up_zero'],\n",
    "#             #     dicPosition['right_up'],\n",
    "#             #     dicPosition['left_up'],  # 'E', 'X', 'N', 'T',\n",
    "#             #     dicPosition['left_zero'],\n",
    "#             #     dicPosition['right_zero'],\n",
    "#             #     dicPosition['left_zero_long'],\n",
    "#             #     dicPosition['left_zero_slight'],  # 'K', 'Q', 'V', 'I',\n",
    "#             #     dicPosition['right_down'],\n",
    "#             #     dicPosition['left_down'],\n",
    "#             #     dicPosition['right_up'],\n",
    "#             #     dicPosition['down_zero']\n",
    "#             # ]  # 'L', 'S', 'G', 'O'\n",
    "#             # for idx in range(len(labels)):\n",
    "#             #     box = dict(boxstyle='round,pad=.5',facecolor='yellow',alpha=0.5)\n",
    "#             #     plt.annotate(labels[idx],\n",
    "#             #                 xy=(recall[idx],precision[idx]), xycoords='data',\n",
    "#             #                 xytext=vecPositions[idx], textcoords='offset points',\n",
    "#             #                 arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n",
    "#             #                 bbox=box)\n",
    "#             if savePath is not None:\n",
    "#                 plt.savefig(os.path.join(savePath, str(classId) + '.png'))\n",
    "#             if showGraphic is True:\n",
    "#                 plt.show()\n",
    "#                 # plt.waitforbuttonpress()\n",
    "#                 plt.pause(0.05)\n",
    "#         return results\n",
    "\n",
    "#     # @staticmethod\n",
    "#     # def CalculateAveragePrecision(rec, prec):\n",
    "#     #     mrec = []\n",
    "#     #     mrec.append(0)\n",
    "#     #     [mrec.append(e) for e in rec]\n",
    "#     #     mrec.append(1)\n",
    "#     #     mpre = []\n",
    "#     #     mpre.append(0)\n",
    "#     #     [mpre.append(e) for e in prec]\n",
    "#     #     mpre.append(0)\n",
    "#     #     for i in range(len(mpre) - 1, 0, -1):\n",
    "#     #         mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
    "#     #     ii = []\n",
    "#     #     for i in range(len(mrec) - 1):\n",
    "#     #         if mrec[1+i] != mrec[i]:\n",
    "#     #             ii.append(i + 1)\n",
    "#     #     ap = 0\n",
    "#     #     for i in ii:\n",
    "#     #         ap = ap + np.sum((mrec[i] - mrec[i - 1]) * mpre[i])\n",
    "#     #     # return [ap, mpre[1:len(mpre)-1], mrec[1:len(mpre)-1], ii]\n",
    "#     #     return [ap, mpre[0:len(mpre) - 1], mrec[0:len(mpre) - 1], ii]\n",
    "\n",
    "#     @staticmethod\n",
    "#     # 11-point interpolated average precision\n",
    "#     # def ElevenPointInterpolatedAP(rec, prec):\n",
    "#     #     # def CalculateAveragePrecision2(rec, prec):\n",
    "#     #     mrec = []\n",
    "#     #     # mrec.append(0)\n",
    "#     #     [mrec.append(e) for e in rec]\n",
    "#     #     # mrec.append(1)\n",
    "#     #     mpre = []\n",
    "#     #     # mpre.append(0)\n",
    "#     #     [mpre.append(e) for e in prec]\n",
    "#     #     # mpre.append(0)\n",
    "#     #     recallValues = np.linspace(0, 1, 11)\n",
    "#     #     recallValues = list(recallValues[::-1])\n",
    "#     #     rhoInterp = []\n",
    "#     #     recallValid = []\n",
    "#     #     # For each recallValues (0, 0.1, 0.2, ... , 1)\n",
    "#     #     for r in recallValues:\n",
    "#     #         # Obtain all recall values higher or equal than r\n",
    "#     #         argGreaterRecalls = np.argwhere(mrec[:] >= r)\n",
    "#     #         pmax = 0\n",
    "#     #         # If there are recalls above r\n",
    "#     #         if argGreaterRecalls.size != 0:\n",
    "#     #             pmax = max(mpre[argGreaterRecalls.min():])\n",
    "#     #         recallValid.append(r)\n",
    "#     #         rhoInterp.append(pmax)\n",
    "#     #     # By definition AP = sum(max(precision whose recall is above r))/11\n",
    "#     #     ap = sum(rhoInterp) / 11\n",
    "#     #     # Generating values for the plot\n",
    "#     #     rvals = []\n",
    "#     #     rvals.append(recallValid[0])\n",
    "#     #     [rvals.append(e) for e in recallValid]\n",
    "#     #     rvals.append(0)\n",
    "#     #     pvals = []\n",
    "#     #     pvals.append(0)\n",
    "#     #     [pvals.append(e) for e in rhoInterp]\n",
    "#     #     pvals.append(0)\n",
    "#     #     # rhoInterp = rhoInterp[::-1]\n",
    "#     #     cc = []\n",
    "#     #     for i in range(len(rvals)):\n",
    "#     #         p = (rvals[i], pvals[i - 1])\n",
    "#     #         if p not in cc:\n",
    "#     #             cc.append(p)\n",
    "#     #         p = (rvals[i], pvals[i])\n",
    "#     #         if p not in cc:\n",
    "#     #             cc.append(p)\n",
    "#     #     recallValues = [i[0] for i in cc]\n",
    "#     #     rhoInterp = [i[1] for i in cc]\n",
    "#     #     return [ap, rhoInterp, recallValues, None]\n",
    "\n",
    "#     # For each detections, calculate IOU with reference\n",
    "#     @staticmethod\n",
    "#     def _getAllIOUs(reference, detections):\n",
    "#         ret = []\n",
    "#         bbReference = reference.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#         # img = np.zeros((200,200,3), np.uint8)\n",
    "#         for d in detections:\n",
    "#             bb = d.getAbsoluteBoundingBox(BBFormat.XYX2Y2)\n",
    "#             iou = Evaluator.iou(bbReference, bb)\n",
    "#             # Show blank image with the bounding boxes\n",
    "#             # img = add_bb_into_image(img, d, color=(255,0,0), thickness=2, label=None)\n",
    "#             # img = add_bb_into_image(img, reference, color=(0,255,0), thickness=2, label=None)\n",
    "#             ret.append((iou, reference, d))  # iou, reference, detection\n",
    "#         # cv2.imshow(\"comparing\",img)\n",
    "#         # cv2.waitKey(0)\n",
    "#         # cv2.destroyWindow(\"comparing\")\n",
    "#         return sorted(ret, key=lambda i: i[0], reverse=True)  # sort by iou (from highest to lowest)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def iou(boxA, boxB):\n",
    "#         # if boxes dont intersect\n",
    "#         if Evaluator._boxesIntersect(boxA, boxB) is False:\n",
    "#             return 0\n",
    "#         interArea = Evaluator._getIntersectionArea(boxA, boxB)\n",
    "#         union = Evaluator._getUnionAreas(boxA, boxB, interArea=interArea)\n",
    "#         # intersection over union\n",
    "#         iou = interArea / union\n",
    "#         assert iou >= 0\n",
    "#         return iou\n",
    "\n",
    "#     # boxA = (Ax1,Ay1,Ax2,Ay2)\n",
    "#     # boxB = (Bx1,By1,Bx2,By2)\n",
    "#     @staticmethod\n",
    "#     def _boxesIntersect(boxA, boxB):\n",
    "#         if boxA[0] > boxB[2]:\n",
    "#             return False  # boxA is right of boxB\n",
    "#         if boxB[0] > boxA[2]:\n",
    "#             return False  # boxA is left of boxB\n",
    "#         if boxA[3] < boxB[1]:\n",
    "#             return False  # boxA is above boxB\n",
    "#         if boxA[1] > boxB[3]:\n",
    "#             return False  # boxA is below boxB\n",
    "#         return True\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _getIntersectionArea(boxA, boxB):\n",
    "#         xA = max(boxA[0], boxB[0])\n",
    "#         yA = max(boxA[1], boxB[1])\n",
    "#         xB = min(boxA[2], boxB[2])\n",
    "#         yB = min(boxA[3], boxB[3])\n",
    "#         # intersection area\n",
    "#         return (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _getUnionAreas(boxA, boxB, interArea=None):\n",
    "#         area_A = Evaluator._getArea(boxA)\n",
    "#         area_B = Evaluator._getArea(boxB)\n",
    "#         if interArea is None:\n",
    "#             interArea = Evaluator._getIntersectionArea(boxA, boxB)\n",
    "#         return float(area_A + area_B - interArea)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _getArea(box):\n",
    "#         return (box[2] - box[0] + 1) * (box[3] - box[1] + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_atom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47bcc98a91e0639070087bf2bbd0f353a7498108652c8107efb5221696e92166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
